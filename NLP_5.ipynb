{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0158c45-e88b-4386-8f65-3f6a167a321c",
   "metadata": {},
   "source": [
    "# Assignment 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c5b96f-982c-45b8-b32c-c125da0a6809",
   "metadata": {},
   "source": [
    "#### 1. What are Sequence-to-sequence models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b83c4e0-a02a-4f20-8226-e05c31caf5c1",
   "metadata": {},
   "source": [
    "Encoder-decoder models, commonly referred to as sequence-to-sequence (seq2seq) models, are a sort of neural network architecture made to handle input and output sequences of different lengths. They are frequently employed for activities including text summarization, speech recognition, machine translation, and conversational agents.\n",
    "\n",
    "An encoder and a decoder are the two primary parts of the seq2seq paradigm.\n",
    "\n",
    "1. Encoder: The encoder processes the input sequence, such as a sentence in machine translation or an audio waveform in speech recognition, and transforms it into a fixed-length vector called the context or thought vector. The encoder typically consists of recurrent neural network (RNN) layers, such as LSTM or GRU, which sequentially process the input elements and capture the context information.\n",
    "\n",
    "2. Decoder: The decoder takes the context vector produced by the encoder and generates the output sequence, such as the translated sentence or the text summary. Like the encoder, the decoder is also composed of RNN layers. It takes the context vector as the initial hidden state and generates the output elements one by one, conditioning on the previously generated elements.\n",
    "\n",
    "During training, the input-output pairs are used to train the model using a technique called teacher forcing, where the decoder is fed with the correct output sequence at each time step. However, during inference or prediction, when the model is used to generate output for new inputs, the decoder uses its own predictions as inputs in an autoregressive manner.\n",
    "\n",
    "The capacity of seq2seq models to handle variable-length sequences is their main benefit. The model can encode the input sequence into a fixed-length context vector and then produce an output sequence of any length by employing an encoder-decoder architecture. Because the length of the input and output sequences may vary, seq2seq models are ideally suited for applications involving sequence-to-sequence mapping.\n",
    "\n",
    "Seq2seq models have been successfully used for a variety of natural language processing tasks, making it possible to create chatbots that respond in a manner resembling that of a human, accurately translate text between languages, and summarise large publications.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5587b39-ae1d-4c68-8af8-65af34f3ca16",
   "metadata": {},
   "source": [
    "#### 2. What are the Problem with Vanilla RNNs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be986596-14d4-41bd-8915-d9df0cc2cf9e",
   "metadata": {},
   "source": [
    "1. Vanishing Gradient Problem: Vanilla RNNs suffer from the vanishing gradient problem, where the gradients diminish exponentially as they propagate through time steps. This issue makes it difficult for the network to capture long-term dependencies in the input sequence, as the influence of distant past inputs diminishes rapidly. It limits the ability of the network to learn and retain information from earlier time steps.\n",
    "\n",
    "2. Exploding Gradient Problem: Conversely, vanilla RNNs can also encounter the exploding gradient problem, where gradients become too large during backpropagation. This issue leads to unstable learning and makes it challenging to update the network's parameters effectively.\n",
    "\n",
    "3. Short-Term Memory: Vanilla RNNs have a limited short-term memory capacity. They struggle to retain relevant information from earlier time steps when processing long sequences. As a result, the network's ability to understand and utilize long-term dependencies is compromised.\n",
    "\n",
    "4. Lack of Parallelization: Vanilla RNNs are inherently sequential models, where each time step depends on the previous time step. This sequential nature limits parallelization, making it difficult to train the model efficiently on hardware with parallel processing capabilities, such as GPUs.\n",
    "\n",
    "5. Difficulty in Capturing Contextual Information: Vanilla RNNs treat each input and time step independently and do not explicitly capture the context or relationship between words or elements in the sequence. This limitation hampers the model's ability to understand the contextual information and dependencies within the input sequence.\n",
    "\n",
    "More sophisticated RNN variations, such LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit) networks, have been created to address these problems. These variations take care of the vanishing gradient issue, improve long-term memory performance, and provide gating mechanisms to regulate information flow. They are now more capable of identifying long-term dependencies and learning from sequential data thanks to these advancements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612adc74-ce45-4b1a-bedb-47790d6c909e",
   "metadata": {},
   "source": [
    "#### 3. What is Gradient clipping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd76a2a-55de-4d49-bf21-44b1db249df4",
   "metadata": {},
   "source": [
    "In recurrent neural networks (RNNs), in particular, gradient clipping is a method for reducing the growing gradient problem in deep neural networks. The exploding gradient problem develops when backpropagation gradients get too big, resulting in unstable training and challenging parameter updates for the network.\n",
    "\n",
    "Gradient clipping is the process of limiting gradient magnitude to a predetermined threshold. The gradients are rescaled or clipped when they go above the threshold to keep their magnitude within a reasonable range. Gradient clipping prevents the gradient from growing too big and producing instability during training by restricting the gradient magnitude.\n",
    "\n",
    "Gradient clipping can be broken down into the following steps:\n",
    "\n",
    "1. Compute the gradients of the network's parameters with respect to the loss function during backpropagation.\n",
    "\n",
    "2. Calculate the L2 norm (Euclidean norm) of the gradient vector to determine its magnitude.\n",
    "\n",
    "3. If the gradient magnitude exceeds a predetermined threshold, rescale the gradient vector by dividing it by its norm to bring it within the desired range. This rescaling ensures that the gradient remains within a manageable magnitude.\n",
    "\n",
    "4. Update the network's parameters using the clipped gradients.\n",
    "\n",
    "Gradient clipping makes the training process more dependable and reduces the likelihood of gradient explosions in the network. As a result, there is no longer any danger of instability due to excessive gradient values preventing the network from learning and adjusting its parameters. When working with deep architectures like RNNs, where the issue of expanding gradients is more common, gradient clipping is extremely helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f71d412-fc60-4137-9098-b8de40d802ed",
   "metadata": {},
   "source": [
    "#### 4. Explain Attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d0fcc0-9ad7-434b-a896-95b4761e6ecd",
   "metadata": {},
   "source": [
    "Many sequence-to-sequence models include an attention mechanism as a crucial element, especially when applied to tasks like neural machine translation and natural language processing. When creating the output sequence, the model benefits from concentrating on pertinent portions of the input sequence.\n",
    "\n",
    "In conventional sequence-to-sequence models, the input sequence is encoded using a fixed-length context vector, which is then sent to the decoder to produce the output sequence. However, especially when dealing with lengthy sequences, this fixed-length representation might not adequately capture the crucial information from the whole input sequence.\n",
    "\n",
    "By enabling the model to dynamically focus on various elements of the input sequence at each stage of the decoding procedure, the attention mechanism addresses this problem. Each input element is given a weight or value based on how pertinent it is to the current decoding phase. These weights, which are sometimes referred to as attention weights, are calculated using a compatibility function that assesses how comparable the hidden states of the encoder and decoder are.\n",
    "\n",
    "The attention mechanism can be summarized in the following steps:\n",
    "\n",
    "1. Compute the attention weights: Calculate the compatibility between the decoder's hidden state and the encoder's hidden states for each input element. The compatibility can be measured using various methods, such as dot product, concatenation, or a neural network.\n",
    "\n",
    "2. Normalize the attention weights: Apply a softmax function to the computed attention weights to ensure they sum up to 1, representing the importance or relevance of each input element.\n",
    "\n",
    "3. Compute the context vector: Multiply the attention weights with the encoder's hidden states to obtain the context vector. The context vector is a weighted sum of the encoder's hidden states, where the weights are determined by the attention weights.\n",
    "\n",
    "4. Combine the context vector with the decoder's hidden state: Concatenate or add the context vector to the decoder's hidden state at the current decoding step. This combined representation is then used to generate the output at that step.\n",
    "\n",
    "The model may selectively focus on various portions of the input sequence by using the attention mechanism to give the relevant elements greater weight while ignoring the unimportant ones. Long-range dependencies are better captured, and the translation or generation quality of the model is enhanced. Additionally, it offers interpretability by emphasising which portions of the input sequence are more important for the production of the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbbc327-76ac-40bb-8173-b1cc7008aef7",
   "metadata": {},
   "source": [
    "#### 5. Explain Conditional random fields (CRFs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43cd6ec-e81f-422d-8d6b-745009a2ee17",
   "metadata": {},
   "source": [
    "Probabilistic graphical models called Conditional Random Fields (CRFs) are employed for structured prediction tasks like sequence labelling and classification. When working with organised or sequential data, where the outcome is dependent on the full input sequence, they are especially effective.\n",
    "\n",
    "A particular kind of discriminative model called a CRF simulates the conditional probability of a series of output labels given a series of input labels. CRFs explicitly model the dependencies between the input and output sequences without assuming any underlying generative process, in contrast to generative models like Hidden Markov Models (HMMs).\n",
    "\n",
    "The main idea behind CRFs is to define a set of features that capture the relevant information from the input sequence, and then model the conditional probability of the output sequence given these features. The features can be local or global, capturing both the local context and the dependencies between neighboring labels.\n",
    "\n",
    "The CRF model is trained using labeled data, where both the input sequence and the corresponding output sequence are known. The model learns the parameters that maximize the conditional likelihood of the output sequence given the input sequence.\n",
    "\n",
    "During inference, given a new input sequence, CRFs use the trained model to find the most likely output sequence. This is typically done using techniques like the Viterbi algorithm or dynamic programming, which efficiently compute the highest-scoring sequence based on the learned parameters.\n",
    "\n",
    "CRFs have several advantages:\n",
    "\n",
    "1. They model the dependencies between output labels, allowing them to capture complex patterns and dependencies in structured data.\n",
    "\n",
    "2. They can handle variable-length input sequences and output sequences.\n",
    "\n",
    "3. They provide a principled probabilistic framework for structured prediction, allowing for uncertainty estimation and confidence scores.\n",
    "\n",
    "4. They can incorporate various types of features, including local, pairwise, and higher-order features, to capture different levels of context and dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9bed94-5b38-4bf1-8d5d-a1cb55fa4f3a",
   "metadata": {},
   "source": [
    "#### 6. Explain self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35fce05-1ead-4bb1-b6f0-8ed4211a2576",
   "metadata": {},
   "source": [
    "Self-attention, often referred to as intra-attention or the self-attention mechanism, is a feature of deep learning models that enables the model to concentrate on various segments of the input sequence while predicting or identifying dependencies. It is especially beneficial for natural language processing (NLP) activities including text summarization, machine translation, and language comprehension.\n",
    "\n",
    "The input sequence is processed sequentially, one piece at a time, in conventional sequence models like recurrent neural networks (RNNs). Since the model's attention focuses only on adjacent elements, long-range dependencies might not be adequately captured by this sequential processing.\n",
    "\n",
    "Self-attention, on the other hand, enables the model to attend to all positions of the input sequence simultaneously. It computes attention weights for each pair of positions in the sequence, determining how much focus should be given to each position when predicting or capturing dependencies.\n",
    "\n",
    "The process of self-attention involves three key components: queries, keys, and values. These are derived from the input sequence and used to compute attention weights:\n",
    "\n",
    "1. Queries: Queries represent the positions or elements in the sequence that are being attended to. For each position, a query vector is computed.\n",
    "\n",
    "2. Keys: Keys are used to compare the compatibility or similarity between different positions. Like queries, key vectors are computed for each position.\n",
    "\n",
    "3. Values: Values hold the information or representations associated with each position in the sequence.\n",
    "\n",
    "The attention weights are computed by taking the dot product between the query vector and the key vector, followed by applying a softmax function to obtain normalized weights. These attention weights determine how much importance is given to each position in the sequence.\n",
    "\n",
    "Finally, the weighted sum of the value vectors, weighted by the attention weights, is computed to obtain the final output or representation for each position in the sequence.\n",
    "\n",
    "Self-attention enables the model to learn long-range dependencies more efficiently by allowing it to recognise relationships between places that are distant from one another in the sequence. Additionally, it enables the model to choose attend to various segments of the sequence while concentrating on the data that is most pertinent to the task at hand.\n",
    "\n",
    "Transformers, a particular sort of neural network design, have attained cutting-edge performance in a number of NLP tasks by largely relying on self-attention mechanisms. In other fields, such as computer vision, self-attention has been utilised to capture spatial relationships between image elements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0555feee-e7fb-47cf-87ea-22dd620d5ff6",
   "metadata": {},
   "source": [
    "#### 7. What is Bahdanau Attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba016dee-0454-4462-a725-915afbdbeb42",
   "metadata": {},
   "source": [
    "Bahdanau The attention mechanism utilised in sequence-to-sequence models, notably in the context of neural machine translation (NMT), is known as attention, also referred to as additive attention. By enabling the model to concentrate on different sections of the source sequence when generating each target word, it was developed by Dzmitry Bahdanau et al. in 2015 to enhance the performance of machine translation models.\n",
    "\n",
    "Traditional sequence-to-sequence approaches summarise the entire source sequence using a fixed-length context vector, which may lead to information loss and make it difficult to capture long-range dependencies. Bahdanau By enabling the model to dynamically pay attention to various portions of the source sequence, attention overcomes this restriction.\n",
    "\n",
    "Following is how the Bahdanau Attention mechanism operates:\n",
    "\n",
    "1. Encoder: The source sequence is passed through an encoder, typically a recurrent neural network (RNN) such as a Long Short-Term Memory (LSTM) or a Gated Recurrent Unit (GRU). The encoder produces a sequence of hidden states, where each hidden state represents the encoded information for a specific position in the source sequence.\n",
    "\n",
    "2. Decoder: The decoder, also an RNN, generates the target sequence word by word. At each decoding step, the decoder's hidden state is updated based on the previously generated word and the previous hidden state.\n",
    "\n",
    "3. Attention Calculation: At each decoding step, the attention mechanism calculates attention weights for each position in the source sequence. These attention weights determine how much importance should be given to each position when generating the current target word. The attention weights are calculated by comparing the current decoder hidden state with the encoder hidden states using a learned compatibility function.\n",
    "\n",
    "4. Context Vector: The attention weights are used to create a weighted sum of the encoder hidden states, resulting in a context vector. The context vector represents the focused information from the source sequence that is relevant for generating the current target word.\n",
    "\n",
    "5. Decoder Output: The context vector is concatenated with the current decoder hidden state and passed through a feed-forward neural network to produce the output for the current decoding step. This output is typically used to predict the probability distribution over the target vocabulary and select the most likely target word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38539a9-4e20-463e-bc41-59bf524953a1",
   "metadata": {},
   "source": [
    "#### 8. What is a Language Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b736de-eeb4-448d-9c73-2f1eae718b87",
   "metadata": {},
   "source": [
    "A language model is a statistical model that discovers the likelihoods of word sequences in a particular language. Its goal is to produce writing that is cohesive and appropriate for the context by capturing the patterns and structures of natural language. A language model is trained using a huge corpus of text data and discovers the likelihood that various words or word combinations will appear in a specific context.\n",
    "\n",
    "The main goal of a language model is to predict the probability distribution over the next word or sequence of words given a history of preceding words. It models the conditional probability of a word given its context, which can be represented as P(w | w1, w2, ..., wn), where w represents the current word and w1, w2, ..., wn represent the previous words in the sequence.\n",
    "\n",
    "Language models can be used for a variety of tasks, including text generation, machine translation, speech recognition, and sentiment analysis. They serve as a fundamental component in many natural language processing (NLP) applications. By learning the statistical patterns and dependencies in language data, language models can generate coherent and contextually appropriate text, complete sentences or phrases, correct spelling and grammar errors, and even assist in understanding human language and generating responses in conversational AI systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049ff065-4b58-4b70-b957-920639166098",
   "metadata": {},
   "source": [
    "#### 9. What is Multi-Head Attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4480b4e-ac46-4e5f-a218-db083e659abd",
   "metadata": {},
   "source": [
    "The attention mechanism utilised in most transformer-based models, including the Transformer architecture, is known as multi-head attention. By using multiple attention heads, it expands the traditional attention mechanism and enables the model to simultaneously attend to various portions of the input sequence and collect a variety of information.\n",
    "\n",
    "The multi-head attention technique is typically employed in the context of natural language processing (NLP) to capture dependencies between words in a sequence. Based on how each word in the input sequence relates to the other words in the sequence, attention weights are computed for each word. The model may pay attention to various facets or viewpoints of the input sequence by employing multiple attention heads, which enables it to learn more varied and complex representations.\n",
    "\n",
    "The multi-head attention mechanism operates in three key steps:\n",
    "\n",
    "1. Linear Projections: The input sequence is projected into multiple subspaces using different learned linear transformations. Each projection generates a separate query, key, and value representation.\n",
    "\n",
    "2. Attention Calculation: For each attention head, the attention weights are computed by comparing the query representations with the key representations of all other words in the sequence. The attention weights determine the importance or relevance of each word in the sequence for the current word being processed.\n",
    "\n",
    "3. Weighted Aggregation: The attention weights are used to compute a weighted sum of the value representations, resulting in the attended representation for each word in the sequence. The attended representations from all the attention heads are concatenated or combined to obtain the final representation for each word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e3f369-ee58-4b27-aaf2-77c0a73442c9",
   "metadata": {},
   "source": [
    "#### 10. What is Bilingual Evaluation Understudy (BLEU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494e1ff0-c7e1-4138-b8c2-9e381f837712",
   "metadata": {},
   "source": [
    "A metric called Bilingual Evaluation Understudy (BLEU) is frequently used to assess the accuracy of machine-translated text. It calculates how closely the machine translation resembles one or more reference translations.\n",
    "\n",
    "The n-gram overlap between the candidate translation and the reference translation(s) is used to calculate the BLEU score. It evaluates the translation's accuracy and conciseness.\n",
    "\n",
    "Here is a detailed description of how BLEU operates:\n",
    "\n",
    "1. Corpus Preparation: First, a corpus of reference translations is created, which consists of multiple reference translations for each sentence or document in the dataset. Each reference translation represents a human-generated translation that is considered as the target translation.\n",
    "\n",
    "2. N-gram Calculation: The candidate translation, which is the output generated by the machine translation system, is segmented into n-grams. An n-gram is a contiguous sequence of n words. Typically, BLEU considers unigrams (single words) and bigrams (two-word sequences), but higher-order n-grams can also be used.\n",
    "\n",
    "3. N-gram Matching: For each n-gram in the candidate translation, the algorithm checks if it occurs in any of the reference translations. If there is a match, it is considered a correct match.\n",
    "\n",
    "4. Precision Calculation: BLEU computes the precision of the candidate translation by counting the total number of correct matches and dividing it by the total number of n-grams in the candidate translation.\n",
    "\n",
    "5. Brevity Penalty: BLEU also penalizes overly short translations. It calculates a brevity penalty based on the length of the candidate translation and the lengths of the closest reference translations. This penalty discourages the generation of very short translations that may artificially inflate the precision score.\n",
    "\n",
    "6. BLEU Score Calculation: Finally, the precision score is combined with the brevity penalty to compute the BLEU score. The precision score is typically multiplied by an exponential function of the brevity penalty to obtain the final BLEU score.\n",
    "\n",
    "Higher BLEU ratings, which range from 0 to 1, indicate better translation quality. A BLEU score of 1 is awarded for a flawless translation.\n",
    "\n",
    "In machine translation research and evaluation, BLEU is frequently utilised. It offers a simple and automatic technique to evaluate the effectiveness of various translation systems using human references. It is crucial to remember that BLEU has some drawbacks, including its dependency on n-gram matching, which might not adequately account for the semantic and syntactic facets of translation quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e32473-f2e1-4ea8-809d-0b89f8e5daa5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
