{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee082d78-0e67-4f7d-a3f1-198d02754444",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503ec598-c7f2-453b-a7e9-f87939c18500",
   "metadata": {},
   "source": [
    "#### 1. Explain One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccd9597-d25e-47e1-b707-486a3e2b9d5a",
   "metadata": {},
   "source": [
    "One-Hot Encoding is a method for representing binary vectors of category variables. It is frequently employed in data preprocessing and machine learning activities, particularly when working with categorical data that cannot be used directly in numerical calculations. \n",
    "\n",
    "Each distinct category in the categorical variable is converted into a binary vector of fixed length in the one-hot encoding method, where only one element is \"hot\" (1) and the others are \"cold\" (0). The number of distinct categories in the variable is equal to the length of the binary vector.\n",
    "\n",
    "Here's an example to illustrate the process:\n",
    "\n",
    "Let's say we have a categorical variable called \"Fruit\" with three categories: \"Apple\", \"Orange\", and \"Banana\". To apply One-Hot Encoding, we would create three binary vectors, one for each category:\n",
    "\n",
    "- \"Apple\" -> [1, 0, 0]\n",
    "- \"Orange\" -> [0, 1, 0]\n",
    "- \"Banana\" -> [0, 0, 1]\n",
    "\n",
    "In the binary vectors, the position of the \"hot\" (1) element corresponds to the category it represents. All other elements are set to \"cold\" (0).\n",
    "\n",
    "One-Hot Encoding allows us to represent categorical variables in a format that can be used as input for machine learning algorithms. Each category becomes a separate feature with binary values, capturing the presence or absence of a specific category in the data. It helps in avoiding any ordinality assumptions between the categories and enables algorithms to treat each category equally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fbd6b7-96e9-417e-aeb1-9dfd052c9754",
   "metadata": {},
   "source": [
    "#### 2. Explain Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232f0ee7-107a-4036-a3b8-b00d30c709f7",
   "metadata": {},
   "source": [
    "Natural language processing (NLP) practitioners frequently utilise the Bag of Words (BoW) technique to express textual data as numbers. It is a quick and efficient approach to turn text into a numerical representation that can be applied to a variety of machine learning applications, including sentiment analysis, document clustering, and text categorization.\n",
    "\n",
    "The Bag of Words method ignores word order and concentrates solely on the frequency of occurrence of each word, treating a text as a collection of words. In order to represent each document as a numerical vector depending on the frequency of these terms, each document is first represented as a \"bag\" or \"vocabulary\" of unique words that are present throughout the whole corpus of documents.\n",
    "\n",
    "Here's a step-by-step explanation of the Bag of Words process:\n",
    "\n",
    "1. Corpus Construction: Gather a collection of documents that form the corpus. These documents can be sentences, paragraphs, or entire text documents.\n",
    "\n",
    "2. Tokenization: Break down each document into individual words or tokens. This step involves removing punctuation, converting text to lowercase, and splitting the text into words based on whitespace or other delimiters.\n",
    "\n",
    "3. Vocabulary Creation: Create a vocabulary or dictionary of unique words present in the corpus. Each unique word is assigned a unique index or identifier.\n",
    "\n",
    "4. Document Representation: Represent each document in the corpus as a numerical vector. The vector has the same length as the vocabulary size, and each element represents the frequency or presence of a word in the document. Various methods can be used to assign values to the vector elements, such as term frequency (count of occurrences) or binary values (presence or absence of the word).\n",
    "\n",
    "5. Vectorization: Combine the numerical vectors of all the documents to create a matrix representation of the entire corpus. Each row in the matrix corresponds to a document, and each column represents a word in the vocabulary.\n",
    "\n",
    "The Bag of Words strategy has some drawbacks. It disregards word order, which results in the loss of crucial context. It also doesn't take into account the semantics of words or the connections between them. Bag of Words is still a popular and efficient method for many text-based machine learning problems in spite of these drawbacks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2319e7f-8f7e-4766-a93e-23a95d655e87",
   "metadata": {},
   "source": [
    "#### 3. Explain Bag of N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d03fe1-e8af-423f-9a97-9485fcdc3eab",
   "metadata": {},
   "source": [
    "Bag of N-Grams is an expansion of the Bag of Words (BoW) approach used in natural language processing (NLP) that takes into account n-grams in addition to single words. Bag of N-Grams measures the frequency of word sequences of length 'n', whereas BoW simply measures the frequency of single words.\n",
    "\n",
    "Here is a detailed breakdown of the Bag of N-Grams procedure:\n",
    "\n",
    "1. Corpus Construction: Gather a collection of documents that form the corpus, similar to the Bag of Words approach.\n",
    "\n",
    "2. Tokenization: Break down each document into individual words or tokens, similar to the Bag of Words approach.\n",
    "\n",
    "3. N-Gram Generation: Generate n-grams from the tokenized documents. An n-gram is a contiguous sequence of 'n' words. For example, if n=2, then the 2-grams (bigrams) of the sentence \"I love to code\" would be \"I love\" and \"love to\" and \"to code\".\n",
    "\n",
    "4. Vocabulary Creation: Create a vocabulary or dictionary of unique n-grams present in the corpus. Each unique n-gram is assigned a unique index or identifier.\n",
    "\n",
    "5. Document Representation: Represent each document in the corpus as a numerical vector based on the frequency or presence of the n-grams. Similar to BoW, various methods can be used to assign values to the vector elements, such as the count of occurrences or binary values indicating the presence or absence of an n-gram.\n",
    "\n",
    "6. Vectorization: Combine the numerical vectors of all the documents to create a matrix representation of the entire corpus, similar to the Bag of Words approach.\n",
    "\n",
    "The Bag of N-Grams method enables the collection of both the frequency of individual words as well as the frequency of word sequences, which can capture some local word order data and add extra context. Taking into account larger n-grams, such as trigrams (n=3) or higher, the model may be able to capture more intricate word patterns and linkages.\n",
    "\n",
    "In a variety of NLP tasks, such as text classification, sentiment analysis, and information retrieval, the bag of N-grams can be utilised as a feature representation. By changing the value of \"n,\" it is a flexible technique that may be modified to collect various amounts of word sequence information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45687997-b842-4eb9-bad6-95dda69013b0",
   "metadata": {},
   "source": [
    "#### 4. Explain TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fa4b07-cebc-4cc0-93c0-55611455a0dc",
   "metadata": {},
   "source": [
    "A term's (word's) relevance in a document within a corpus of documents is measured using the TF-IDF (Term Frequency-Inverse Document Frequency) statistic in natural language processing (NLP). It seeks to draw attention to words that, when compared to the total corpus, are more significant in a particular document.\n",
    "\n",
    "Term frequency, or TF, refers to how frequently a term (or word) appears in a document. It determines the proportion between a term's occurrences and the overall number of terms in a document. It aids in locating the words that are used the most frequently in a manuscript.\n",
    "\n",
    "IDF (Inverse Document Frequency) measures the rarity or uniqueness of a term across the entire corpus. It calculates the logarithm of the ratio between the total number of documents in the corpus and the number of documents containing the term. IDF assigns higher weights to terms that appear in a smaller number of documents, considering them more informative or distinctive.\n",
    "\n",
    "The TF-IDF score of a term in a document is obtained by multiplying its TF and IDF values. The higher the TF-IDF score of a term in a document, the more important or relevant that term is to the document.\n",
    "\n",
    "Here's the calculation formula for TF-IDF:\n",
    "\n",
    "TF-IDF(term, document) = TF(term, document) * IDF(term)\n",
    "\n",
    "The TF-IDF can be used to represent documents as numerical feature vectors, with each dimension denoting a distinct term and the value indicating the term's TF-IDF score. These feature vectors can be utilised in a variety of NLP tasks, including text mining, document classification, and information retrieval, to determine how similar two texts are to one another or to locate crucial terms within a document.\n",
    "\n",
    "Common terms that appear in most texts, such as \"the,\" \"is,\" and \"and,\" are less influential thanks to TF-IDF, which also helps to emphasise uncommon or distinctive terms that are more semantically significant or provide context for a given document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445f9aed-4b51-458b-a8bb-b8bfda24276c",
   "metadata": {},
   "source": [
    "#### 5. What is OOV problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fe62b9-0b7e-429c-b368-c7b0b5fe13be",
   "metadata": {},
   "source": [
    "The OOV (Out-of-Vocabulary) problem is the difficulty in managing words or tokens that are used in language processing tasks but are not from a model or system's training set or vocabulary. Since the model has no prior knowledge of or representation for an OOV term, it presents challenges when it is encountered.\n",
    "\n",
    "The OOV issue can occur in a variety of natural language processing (NLP) activities, including text categorization, sentiment analysis, speech recognition, and machine translation. There are a number of causes for it, including:\n",
    "\n",
    "1. Rare or infrequent words: If a word appears rarely in the training data or is entirely absent, the model may not have learned any meaningful representation for it.\n",
    "\n",
    "2. Out-of-domain or domain-specific words: If the training data does not cover specific domains or topics, words from those domains may be treated as OOV when encountered during inference.\n",
    "\n",
    "3. Misspellings or variations: Words that are misspelled, abbreviated, or written in a different form from what was seen during training can be considered OOV.\n",
    "\n",
    "The OOV problem can impact the performance and accuracy of NLP models, as they may struggle to handle or make sense of unseen words. Handling the OOV problem typically involves implementing strategies such as:\n",
    "\n",
    "1. Handling unknown words: Assigning a special token or placeholder for OOV words during inference and treating them as a distinct category.\n",
    "\n",
    "2. Word normalization: Applying techniques like stemming, lemmatization, or handling case sensitivity to bring words into a more standardized form and reduce the chances of encountering OOV words.\n",
    "\n",
    "3. Incorporating external resources: Utilizing pre-trained word embeddings or language models that have a larger vocabulary and coverage, which can provide representations for a broader range of words.\n",
    "\n",
    "4. Data augmentation: Expanding the training data by generating or augmenting examples with variations, synonyms, or domain-specific terms to expose the model to a wider vocabulary.\n",
    "\n",
    "5. Dynamic vocabulary expansion: Updating the vocabulary of the model dynamically as new words are encountered during inference, either by adding them to the existing vocabulary or by using open-vocabulary models that can handle OOV words more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc503a6-7b86-4e73-a5c3-fa748229dc4e",
   "metadata": {},
   "source": [
    "#### 6. What are word embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7856173c-634c-467f-8145-87e021c295cc",
   "metadata": {},
   "source": [
    "Word embeddings are numerically based distributed representations of words that identify syntactic and semantic links between words. They are dense vector representations, where vectors near to each other in the vector space represent words with related meanings or contextual usage.\n",
    "\n",
    "Large volumes of text data are frequently used to train neural network models like Word2Vec, GloVe, or FastText to learn word embeddings. Based on the context of the training data, these models attempt to capture the meaning of words. The generated word embeddings represent the connections and commonalities in word meanings.\n",
    "\n",
    "The following are some benefits of using word embeddings for natural language processing (NLP) tasks:\n",
    "\n",
    "1. Dimensionality reduction: Word embeddings typically have a lower dimensionality compared to one-hot encoded word representations, which reduces the computational complexity and memory requirements of NLP models.\n",
    "\n",
    "2. Semantic relationships: Word embeddings capture semantic relationships between words. For example, words with similar meanings or that often appear in similar contexts have embeddings that are close together in the vector space.\n",
    "\n",
    "3. Analogical reasoning: Word embeddings can exhibit interesting algebraic relationships. For example, by performing vector operations such as addition and subtraction on word embeddings, it is possible to find analogies like \"king - man + woman = queen.\"\n",
    "\n",
    "4. Generalization: Word embeddings can generalize well to unseen words or rare words that were not present in the training data. By leveraging the context and relationships learned during training, word embeddings can provide meaningful representations for unseen words.\n",
    "\n",
    "Many NLP applications, such as sentiment analysis, machine translation, text classification, named entity identification, and others, now use word embeddings as a fundamental building block. By utilising contextual data and the semantic connections between words, they enable models to catch the subtle semantic variations of words and enhance the performance of NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6305fd74-7e3a-4151-9d1f-c0a50f37cfa7",
   "metadata": {},
   "source": [
    "#### 7. Explain Continuous bag of words (CBOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed11b8a-f442-40ce-a4c4-d2d52394a760",
   "metadata": {},
   "source": [
    "A well-liked approach for word embedding training in natural language processing (NLP) is Continuous Bag of Words (CBOW). A target word's context, which consists of the words around it in a phrase or a fixed-size window, is what CBOW attempts to forecast.\n",
    "\n",
    "With the aid of a neural network design, the CBOW model is trained. This is how it goes:\n",
    "\n",
    "1. Data Preparation: CBOW requires a large corpus of text as training data. The corpus is split into sentences, and a sliding window of a fixed size is used to create training samples. Each training sample consists of the context words as input and the target word as the output.\n",
    "\n",
    "2. Word Encoding: Before training, each word in the vocabulary is assigned a unique index. The words are often represented as one-hot encoded vectors or through other encoding schemes.\n",
    "\n",
    "3. Architecture: CBOW uses a shallow neural network with a single hidden layer. The input layer has neurons equal to the size of the context window, and the output layer has neurons equal to the size of the vocabulary.\n",
    "\n",
    "4. Training: During training, the CBOW model learns to predict the target word given its context. The context words are fed into the input layer, and their embeddings (word vectors) are averaged to obtain the context representation. This context representation is then passed through the hidden layer and finally through the output layer, which outputs the probabilities of each word in the vocabulary.\n",
    "\n",
    "5. Loss Calculation: The model's predictions are compared to the true target word using a loss function such as cross-entropy. The model's parameters (weights and biases) are adjusted through backpropagation and gradient descent to minimize the loss.\n",
    "\n",
    "6. Word Embeddings: Once the CBOW model is trained, the weights of the hidden layer (context representation) serve as word embeddings. These embeddings capture the distributional properties of words in the training corpus, representing their semantic and syntactic relationships.\n",
    "\n",
    "CBOW is renowned for its effectiveness and capacity to produce high-quality word embeddings, particularly for terms that are often used in the training set. It is especially helpful in situations where the context offers clear hints about what the target term means. It might not, however, capture long-range dependencies or handle uncommon terms as well as other models, such as Skip-gram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bfcf5b-6fd1-4ee1-9a84-c3e03d770e51",
   "metadata": {},
   "source": [
    "#### 8. Explain SkipGram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db810e15-702a-4027-9b67-55a3ed0ec1e0",
   "metadata": {},
   "source": [
    "Another well-liked approach for word embedding training in natural language processing (NLP) is the skip-gram. Skip-gram predicts the context words given a target word, as contrast to Continuous Bag of Words (CBOW), which predicts the target word given its context.\n",
    "\n",
    "The Skip-gram model operates as follows:\n",
    "\n",
    "1. Data Preparation: Similar to CBOW, the training data for Skip-gram consists of a large corpus of text. The corpus is split into sentences, and a sliding window is used to create training samples. Each training sample consists of a target word and its context words within a fixed-size window.\n",
    "\n",
    "2. Word Encoding: Each word in the vocabulary is assigned a unique index, and the words are often represented as one-hot encoded vectors or through other encoding schemes.\n",
    "\n",
    "3. Architecture: Skip-gram also uses a shallow neural network, but with a different architecture compared to CBOW. The input layer has neurons equal to the size of the vocabulary, and the hidden layer has a lower dimensionality, typically referred to as the embedding size.\n",
    "\n",
    "4. Training: During training, the Skip-gram model learns to predict the context words given a target word. The target word is fed into the input layer, and its embedding (word vector) is obtained from the hidden layer. This embedding is then passed through the output layer, which produces a probability distribution over the vocabulary. The model aims to maximize the probability of the true context words and minimize the probability of other words.\n",
    "\n",
    "5. Loss Calculation: The loss function used in Skip-gram is typically a form of softmax loss or negative sampling. Softmax loss calculates the cross-entropy between the predicted probabilities and the true context words, while negative sampling randomly selects negative examples and adjusts the model's parameters to distinguish between true context words and randomly sampled words.\n",
    "\n",
    "6. Word Embeddings: After training, the weights of the hidden layer serve as word embeddings. These embeddings capture the semantic and syntactic relationships between words in the training corpus. The embeddings are dense vector representations that can be used for various NLP tasks, such as word similarity, document classification, and language generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6577bcce-7a15-419a-8438-69a90d54449f",
   "metadata": {},
   "source": [
    "#### 9. Explain Glove Embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3ee119-80c6-4da6-8c0e-1a8d263ac2d0",
   "metadata": {},
   "source": [
    "GloVe (Global Vectors for Word Representation) is a word embedding model that uses co-occurrence data from a sizable corpus of text to attempt to capture semantic and grammatical relationships between words. Researchers at Stanford University created it.\n",
    "\n",
    "GloVe embeddings operate as follows:\n",
    "\n",
    "1. Co-occurrence Matrix: GloVe starts by constructing a co-occurrence matrix from the corpus of text. The matrix captures how often each word co-occurs with other words in a given window of context. The co-occurrence count represents the strength of the relationship between words.\n",
    "\n",
    "2. Probability Distribution: The co-occurrence matrix is transformed into a probability distribution by normalizing the counts. This step accounts for the frequency biases in the raw counts and focuses on the relative importance of word co-occurrences.\n",
    "\n",
    "3. Word Embeddings: GloVe aims to learn word embeddings that encode semantic and syntactic information based on the co-occurrence probabilities. It does this by minimizing a loss function that measures the difference between the dot product of word embeddings and the logarithm of their co-occurrence probabilities. The embeddings are initialized randomly and updated iteratively during training to optimize the loss.\n",
    "\n",
    "4. Training: The training process involves adjusting the word embeddings iteratively using gradient descent optimization. The goal is to learn word representations that capture the semantic relationships between words. The embeddings are updated based on their influence on the co-occurrence probabilities.\n",
    "\n",
    "5. Vector Space Representation: The resulting GloVe embeddings are dense vector representations where each dimension corresponds to a latent feature capturing word properties. Words with similar meanings or usage tend to have similar embeddings, allowing for semantic relationships to be captured in the vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6eb184-3106-49d1-9dd3-75d8b1463be8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
