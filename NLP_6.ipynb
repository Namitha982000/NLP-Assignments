{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7af9f076-0b02-4718-8dc5-50fb588977e5",
   "metadata": {},
   "source": [
    "# Assignment 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57ae4b0-0d23-4c56-a6ba-771e14acac2d",
   "metadata": {},
   "source": [
    "#### 1. What are Vanilla autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc704808-aedd-4b40-9959-2d3386f26fda",
   "metadata": {},
   "source": [
    "Vanilla autoencoders are a form of neural network architecture used for unsupervised learning and dimensionality reduction. They are also referred to as basic autoencoders or classic autoencoders. In order to reconstruct the original input from the encoded representation, they try to learn a compressed form of the input data.\n",
    "\n",
    "An encoder and a decoder make up the two primary components of a standard autoencoder. The encoder converts the input data into a representation in a lower-dimensional latent space. The key characteristics or patterns in the input data are captured by this latent representation. The original input data is subsequently recreated by the decoder using this encoded representation.\n",
    "\n",
    "The encoding process in the autoencoder involves transforming the input data through a series of hidden layers, typically fully connected layers. These layers reduce the dimensionality of the data by applying non-linear transformations and learning meaningful representations. The decoder, on the other hand, mirrors the structure of the encoder but in reverse, transforming the encoded representation back to the original input space.\n",
    "\n",
    "During training, the autoencoder aims to minimize the reconstruction error between the input data and the output of the decoder. This is typically done by using a loss function such as mean squared error (MSE) or binary cross-entropy. By minimizing this error, the autoencoder learns to capture the most important features of the input data in the latent space.\n",
    "\n",
    "Vanilla autoencoders can be used for various purposes, including dimensionality reduction, data compression, and denoising. They can also serve as a building block for more complex architectures, such as variational autoencoders (VAEs) and generative adversarial networks (GANs). However, one limitation of vanilla autoencoders is that they may struggle to capture complex or high-dimensional patterns in the data compared to more advanced architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b387ee-5aec-4ee2-bb95-7233359ea657",
   "metadata": {},
   "source": [
    "#### 2. What are Sparse autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42fade2-22fc-43c0-8648-402343488e57",
   "metadata": {},
   "source": [
    "One kind of autoencoder, known as a sparse autoencoder, places restrictions on the hidden layer activations during training. The objective of sparse autoencoders is to promote sparsity, which means that only a small number of hidden units are active at any given time, while learning a compressed representation of the input data.\n",
    "\n",
    "The sparsity restriction in a sparse autoencoder is often met by including a regularisation component in the loss function. By penalising the units' average activation, this regularisation term promotes the concealed units to have less activations. The L1 regularisation, which encourages the hidden units to have numerous zero activations, is the regularisation term that is most frequently employed.\n",
    "\n",
    "The sparsity constraint in sparse autoencoders has several benefits. Firstly, it helps in learning more meaningful and efficient representations of the input data. By promoting sparsity, the autoencoder is forced to select the most relevant features for encoding the data. This can lead to better generalization and improved performance on tasks such as classification or reconstruction.\n",
    "\n",
    "Secondly, sparse autoencoders can be useful for handling high-dimensional data with many irrelevant or noisy features. By enforcing sparsity, the autoencoder focuses on capturing the most important features while ignoring irrelevant ones. This can help in reducing overfitting and improving the efficiency of the learning process.\n",
    "\n",
    "Sparse autoencoders have been applied in various domains, including image and text data analysis, anomaly detection, and feature learning. They provide a way to learn compact representations of data while encouraging sparsity, which can lead to more interpretable and efficient models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4146c417-aff9-4aea-ae5f-d6293149750f",
   "metadata": {},
   "source": [
    "#### 3. What are Denoising autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ebb56e-a67f-426d-aeaf-6960814cb897",
   "metadata": {},
   "source": [
    "An autoencoder that can handle noisy or faulty input data is called a denoising autoencoder. By reconstructing the original input from its noisy counterpart, denoising autoencoders aim to develop a representation of the underlying clean data.\n",
    "\n",
    "In a denoising autoencoder, noise or other disturbances are purposefully added to the input data to corrupt it. The autoencoder, which has been taught to reconstruct the original, clean data, is then fed the corrupted data. The concept is that the autoencoder can capture the fundamental structure and properties of the data, thereby denoising it, by learning to retrieve the clean data from the corrupted form.\n",
    "\n",
    "To train a denoising autoencoder, a loss function is defined to measure the discrepancy between the reconstructed output and the original clean input. The model is then optimized to minimize this reconstruction loss, adjusting the weights and biases of the autoencoder's layers to improve the reconstruction performance.\n",
    "\n",
    "Denoising autoencoders have several advantages and applications. Firstly, they can be used to denoise and recover corrupted or noisy data, making them useful for tasks such as image denoising, speech denoising, or removing noise from sensor data.\n",
    "\n",
    "Secondly, denoising autoencoders can also be seen as a form of regularization. By training the autoencoder to reconstruct the clean data from the corrupted version, the model is forced to learn more robust and generalizable representations of the data. This can help in reducing overfitting and improving the model's ability to generalize to unseen data.\n",
    "\n",
    "Denoising autoencoders are a powerful tool for learning meaningful representations from noisy or corrupted data. They have been applied in various domains and can be used for tasks such as data denoising, feature learning, and data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0b067a-0a9c-4f64-8705-8edeca84aa23",
   "metadata": {},
   "source": [
    "#### 4. What are Convolutional autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbd5f04-10c0-4bbe-8580-40624e5f1041",
   "metadata": {},
   "source": [
    "Convolutional neural networks (CNNs) are a type of autoencoder that are used as the fundamental architecture in convolutional autoencoders. They are made especially for handling and reassembling image data.\n",
    "\n",
    "Convolutional layers, pooling layers, and maybe other sorts of layers such batch normalisation or activation functions make up the encoder portion of a convolutional autoencoder. These layers convert the input image into a representation with less dimensions while preserving the key details and patterns.\n",
    "\n",
    "The convolutional autoencoder's decoder component has an architecture similar to the encoder, but in reverse. It is made up of upsampling layers, convolutional transpose layers (sometimes called deconvolutional layers), and other relevant layers. By gradually upsampling and decoding the features back to the original image dimensions, the decoder reconstructs the image from the encoded representation.\n",
    "\n",
    "During training, the convolutional autoencoder is optimized to minimize the reconstruction error between the original input image and the reconstructed output. This is typically done using a loss function such as mean squared error (MSE) or binary cross-entropy.\n",
    "\n",
    "Convolutional autoencoders have several advantages for image-related tasks. They are able to capture spatial dependencies in the image data through the convolutional layers, making them well-suited for tasks such as image denoising, image inpainting, and image compression. The convolutional nature of the autoencoder also allows for parameter sharing, enabling efficient learning and representation of large images.\n",
    "\n",
    "Furthermore, convolutional autoencoders can also be used for unsupervised feature learning. By training the autoencoder to reconstruct images, the encoder part of the network learns meaningful and compact representations of the input data. These learned representations can then be used as features for other downstream tasks like image classification or object detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829ccc10-004c-4fcf-8a23-39b474b0010a",
   "metadata": {},
   "source": [
    "#### 5. What are Stacked autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f600dead-46d3-4223-95d5-2095c0fba756",
   "metadata": {},
   "source": [
    "A sort of neural network architecture called stacked autoencoders, often referred to as deep autoencoders, is made up of several layers of autoencoders placed on top of one another. A hierarchical representation of the input data is produced by each layer of the stack, which serves as both an encoder and a decoder.\n",
    "\n",
    "The raw input data is given to the first layer of the stacked autoencoder, which then learns a compressed version of it. The following layer then receives this compressed form as input and learns an even more condensed representation. A complex hierarchy of encoded representations is created as a result of repeating this process for each additional layer.\n",
    "\n",
    "During the training process, stacked autoencoders are typically trained layer by layer in an unsupervised manner. Each layer is trained as a separate autoencoder, with the objective of reconstructing the input data from the encoded representation. Once the individual layers are trained, the entire stack of layers is fine-tuned using backpropagation to improve the overall reconstruction performance.\n",
    "\n",
    "The main advantage of stacked autoencoders is their ability to learn increasingly abstract and hierarchical representations of the input data. Each layer in the stack learns to extract and encode higher-level features based on the representations learned by the previous layers. This hierarchical representation learning can be beneficial for tasks such as dimensionality reduction, feature extraction, and unsupervised pretraining for supervised learning tasks.\n",
    "\n",
    "Stacked autoencoders have been widely used for various applications, including image recognition, natural language processing, and recommendation systems. By leveraging the power of deep neural networks, stacked autoencoders can capture complex patterns and dependencies in the data, enabling more effective learning and representation of the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e40a3a-c64d-4a1e-86d5-e1e1197927e8",
   "metadata": {},
   "source": [
    "#### 6. Explain how to generate sentences using LSTM autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a07f4b-df1d-4bb7-bb09-553ec7c632e5",
   "metadata": {},
   "source": [
    "To generate sentences using LSTM autoencoders, you can follow these steps:\n",
    "\n",
    "1. Prepare the training data: Gather a dataset of sentences that will be used to train the LSTM autoencoder. Each sentence should be tokenized into individual words or subword units.\n",
    "\n",
    "2. Preprocess the data: Convert the words or subword units into numerical representations that can be fed into the LSTM autoencoder. This can be done using techniques like word embedding or one-hot encoding.\n",
    "\n",
    "3. Build the LSTM autoencoder: Construct an LSTM autoencoder architecture with an encoder and a decoder. The encoder LSTM takes the input sentences and learns a compressed representation (encoding) of the input data. The decoder LSTM reconstructs the input sentences from the encoded representation.\n",
    "\n",
    "4. Train the LSTM autoencoder: Train the LSTM autoencoder using the prepared training data. The objective is to minimize the reconstruction error between the input sentences and the reconstructed sentences generated by the decoder LSTM.\n",
    "\n",
    "5. Encode the input sentence: Once the LSTM autoencoder is trained, encode an input sentence using the trained encoder LSTM. This will produce a compressed representation of the input sentence.\n",
    "\n",
    "6. Generate sentences: Start with the encoded representation from step 5 and use it as the initial state of the decoder LSTM. Generate each word in the output sentence one by one by feeding the output of the decoder LSTM from the previous time step as the input for the next time step. Repeat this process until the desired length of the generated sentence is reached.\n",
    "\n",
    "7. Decode the output: Convert the numerical representation of the generated sentence back into words or subword units using the reverse process of the data preprocessing step.\n",
    "\n",
    "8. Repeat steps 5-7 for multiple iterations or until a desired number of sentences are generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67520489-c46c-49fb-9024-b58eb3fd7caa",
   "metadata": {},
   "source": [
    "#### 7. Explain Extractive summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddff6c33-ea0d-46ba-8491-b664db8d5c2c",
   "metadata": {},
   "source": [
    "By choosing and removing the most significant or pertinent lines or phrases from the original content, extractive summarising is a technique used to create a summary of a document or text. Extractive summarization recognises and keeps the important information from the source text rather than creating new phrases.\n",
    "\n",
    "The following is a general extractive summary procedure:\n",
    "\n",
    "1. Text Preprocessing: Clean the input text by removing any unnecessary characters, stopwords, or special symbols. Perform tokenization to break the text into individual words or phrases.\n",
    "\n",
    "2. Sentence Scoring: Assign a score to each sentence in the text based on various criteria, such as word frequency, position within the document, importance of the words, or similarity to important terms.\n",
    "\n",
    "3. Sentence Selection: Select the top-ranked sentences based on their scores. The number of selected sentences may be predetermined or adjusted based on the desired length of the summary.\n",
    "\n",
    "4. Summary Generation: Combine the selected sentences to form a coherent summary that captures the main points or essential information from the original text.\n",
    "\n",
    "Extractive summarization has several advantages:\n",
    "\n",
    "1. Retains Source Information: Extractive summarization directly extracts sentences from the source text, ensuring that the summary contains accurate and factual information from the original content.\n",
    "\n",
    "2. Preserves Language Style: Since extractive summarization utilizes sentences directly from the source text, it retains the writing style and linguistic nuances of the original document.\n",
    "\n",
    "3. Interpretable and Reliable: The extracted sentences can be easily understood and verified as they are taken directly from the source. This makes the summary more interpretable and reliable.\n",
    "\n",
    "However, extractive summarization also has limitations:\n",
    "\n",
    "1. Lack of Cohesion: Extracted sentences may not always flow smoothly together, resulting in a summary that lacks overall coherence.\n",
    "\n",
    "2. Redundancy: Some sentences may contain redundant information, leading to repetition within the summary.\n",
    "\n",
    "3. Limited Creativity: Extractive summarization relies on existing sentences and phrases, which restricts the generation of novel or unique content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dda645-a743-47c3-8e88-ec6031dbade2",
   "metadata": {},
   "source": [
    "#### 8. Explain Abstractive summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb50535-044b-411c-9e55-0f4ceafe3094",
   "metadata": {},
   "source": [
    "Using new sentences or phrases that more succinctly and naturally reflect the core of the original information, abstractive summarization creates a summary of a document or text. In contrast to extractive summarization, which picks out and extracts sentences from the source text, abstractive summary entails deciphering the text's meaning and coming up with new sentences that highlight the important points.\n",
    "\n",
    "The following is a standard procedure for abstractive summarization:\n",
    "\n",
    "1. Text Understanding: Utilize natural language processing (NLP) techniques to analyze and understand the meaning of the source text. This may involve tasks such as part-of-speech tagging, named entity recognition, and semantic analysis.\n",
    "\n",
    "2. Content Compression: Identify the most important concepts, ideas, or entities in the source text and condense them into shorter representations. This could involve paraphrasing or rephrasing the original content to express the same information using fewer words.\n",
    "\n",
    "3. Language Generation: Use techniques such as language modeling, recurrent neural networks (RNNs), or transformer models (e.g., GPT) to generate new sentences that capture the key information from the source text. This involves leveraging the learned patterns, grammar, and context to produce coherent and fluent summaries.\n",
    "\n",
    "4. Summary Refinement: Refine the generated summary by ensuring coherence, readability, and overall quality. This may involve post-processing steps such as grammar correction, sentence reordering, or removing redundant information.\n",
    "\n",
    "Abstractive summarization has several advantages:\n",
    "\n",
    "1. Creativity: Abstractive summarization has the ability to generate new sentences and phrases, allowing for more creativity and flexibility in expressing the essence of the original content.\n",
    "\n",
    "2. Coherence and Readability: Abstractive summarization can generate summaries that flow more naturally and exhibit better overall coherence compared to extractive approaches. This leads to summaries that are easier to read and understand.\n",
    "\n",
    "3. Conciseness: Abstractive summarization can capture the main points of the text using fewer words by condensing information and removing redundant details.\n",
    "\n",
    "However, abstractive summarization also faces challenges:\n",
    "\n",
    "1. Information Accuracy: Since abstractive summarization involves generating new sentences, there is a risk of introducing inaccuracies or misinterpretations in the summary.\n",
    "\n",
    "2. Language Fluency: Generating coherent and fluent sentences that mimic human-like writing style and grammar is a challenging task, and the quality of the generated summaries may vary.\n",
    "\n",
    "3. System Complexity: Abstractive summarization techniques often rely on advanced NLP and deep learning models, which can be computationally expensive and require large amounts of training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fab0f9f-5169-411e-be54-eec7eb7982b7",
   "metadata": {},
   "source": [
    "#### 9. Explain Beam search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2135479b-22d1-4f77-9603-7df6af95a746",
   "metadata": {},
   "source": [
    "Beam search is a search algorithm commonly used in natural language processing tasks, such as machine translation or text generation, to find the most likely sequence of output tokens given a sequence of input tokens. It is particularly useful in scenarios where the space of possible output sequences is large and exhaustively exploring all possibilities is not feasible.\n",
    "\n",
    "Beam search functions by maintaining a set of partial output sequences, referred to as the beam, and iteratively expanding these sequences by taking into account a number of potential possibilities at each step. The number of candidate sequences to monitor at each stage depends on the beam size.\n",
    "\n",
    "Here is a general explanation of beam search:\n",
    "\n",
    "1. Initialization: Start with a beam of size k and initialize it with a special start token as the first input.\n",
    "\n",
    "2. Generation Step: For each sequence in the current beam, generate the next set of candidate tokens using the underlying language model. These candidates represent potential extensions to the existing sequences.\n",
    "\n",
    "3. Score Calculation: Assign a score to each candidate token based on its probability according to the language model. This score can also consider other factors such as length normalization or penalty for repeated tokens.\n",
    "\n",
    "4. Beam Selection: Select the top k candidates with the highest scores and keep them as the updated beam for the next step. These candidates will become the starting point for generating the next set of candidate tokens.\n",
    "\n",
    "5. Repeat: Repeat steps 2 to 4 until a maximum length is reached or until a special end token is generated.\n",
    "\n",
    "6. Termination: Once the generation process is completed, select the sequence with the highest overall score from the final beam as the output sequence.\n",
    "\n",
    "The key aspect of beam search is that it explores multiple candidate sequences simultaneously, allowing for a more comprehensive search of the solution space. By keeping track of the most promising sequences at each step, beam search provides a trade-off between exploration and exploitation, balancing the desire for high-quality sequences while considering a diverse set of options.\n",
    "\n",
    "Beam search offers several advantages:\n",
    "\n",
    "1. Efficiency: It reduces the computational complexity compared to exhaustive search by limiting the number of sequences considered at each step.\n",
    "\n",
    "2. Exploration: Beam search allows for exploring multiple possible paths, potentially capturing diverse solutions.\n",
    "\n",
    "3. Flexibility: The beam size can be adjusted to control the trade-off between exploration and exploitation. A larger beam size increases the search space but also improves the chances of finding better solutions.\n",
    "\n",
    "However, beam search also has some limitations:\n",
    "\n",
    "1. Sub-optimality: Beam search can lead to suboptimal solutions, especially if the highest-scoring sequences at each step do not necessarily lead to the best overall sequence.\n",
    "\n",
    "2. Repetition: Beam search may be prone to generating repetitive or redundant sequences, as it tends to favor locally optimal choices at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7226f25b-d03e-4646-947b-c5289caae9fa",
   "metadata": {},
   "source": [
    "#### 10. Explain Length normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6958bd-bdbd-4182-bebf-685d424012d1",
   "metadata": {},
   "source": [
    "In order to counteract the bias towards shorter sequences in scoring or ranking-based models, length normalisation is a frequent technique used in tasks that require the production of sequences, such as machine translation or text summarization. The goal of length normalisation is to ensure fairness when comparing or evaluating various sequences of varied lengths by accounting for the inherent length disparities in output sequences.\n",
    "\n",
    "Because of the nature of the modelling process, models that create sequences frequently favour shorter sequences because they frequently have better probability or scores. This bias can provide less-than-ideal outcomes, particularly when assessing or contrasting sequences of various durations. By altering the scores or probability in accordance with the length of the sequence, length normalisation aids in resolving this problem.\n",
    "\n",
    "One common length normalization technique is to divide the score or probability of a sequence by its length. This division operation normalizes the score to account for the impact of sequence length. By dividing the score by the length, longer sequences are penalized, and shorter sequences are rewarded, ensuring a fair comparison between sequences of different lengths.\n",
    "\n",
    "The division can be performed using various formulas, such as:\n",
    "\n",
    "1. Simple Length Normalization: Divide the score by the length of the sequence.\n",
    "   Normalized Score = Score / Length\n",
    "\n",
    "2. Log-Length Normalization: Take the logarithm of the length and then divide the score.\n",
    "   Normalized Score = Score / log(Length)\n",
    "\n",
    "3. Square Root-Length Normalization: Take the square root of the length and then divide the score.\n",
    "   Normalized Score = Score / sqrt(Length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e2b8d3-4dfd-41ec-a1c2-d114d0a50bdb",
   "metadata": {},
   "source": [
    "#### 11. Explain Coverage normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460dca22-5dbc-47a0-8fc3-575aa863272a",
   "metadata": {},
   "source": [
    "In sequence-to-sequence models, notably in the context of abstractive text summarization, the approach of coverage normalisation is employed to promote the development of summaries that cover significant information from the original text. Due to the shortcomings of the underlying model, it addresses the issue of summaries being generated that can omit or duplicate significant content.\n",
    "\n",
    "The original text is encoded into a fixed-length representation during abstractive summarization, such as the hidden state of an LSTM or the result of an attention mechanism. The summary is then decoded using this representation as the starting point. The model pays attention to various areas of the source text when decoding it in order to compile pertinent data and provide the summary tokens.\n",
    "\n",
    "Coverage normalization introduces an additional mechanism to track the coverage of the source text during decoding. It keeps a coverage vector that records the attention distribution over the source text at each time step. The coverage vector is updated by adding the attention weights from the current time step to the previous coverage vector.\n",
    "\n",
    "The coverage vector serves two purposes:\n",
    "\n",
    "1. Attention Distribution: The coverage vector influences the attention mechanism during decoding. It encourages the model to attend to parts of the source text that have not been covered well so far. By doing so, it helps prevent repeated or redundant generation of summary tokens and promotes more comprehensive coverage of the source text.\n",
    "\n",
    "2. Coverage Loss: The coverage vector is used to calculate a coverage loss, which is added to the regular loss function during training. The coverage loss measures the extent to which the attention distribution deviates from a uniform distribution over the source text. By minimizing the coverage loss, the model learns to evenly distribute attention over the source text and reduce any biases or gaps in coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374b75ca-c506-4881-9675-4ebefa387603",
   "metadata": {},
   "source": [
    "#### 12. Explain ROUGE metric evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706a3f6f-b1bc-46eb-8c68-4af4a8925879",
   "metadata": {},
   "source": [
    "A collection of evaluation measures known as ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is frequently used to rate the effectiveness of text summarization software. The comparison of the overlap between the generated summary and one or more reference summaries is the main focus of these measures.\n",
    "\n",
    "Based on the amount of overlapping n-grams (contiguous word sequences) between the generated summary and the reference summary, the ROUGE metrics compute numerous metrics. ROUGE-N, ROUGE-L, and ROUGE-S are the three most popular ROUGE measures.\n",
    "\n",
    "1. ROUGE-N: This metric measures the overlap of n-grams between the generated summary and the reference summary. It computes precision, recall, and F1-score for different values of n (typically 1, 2, or 3). The higher the ROUGE-N scores, the better the quality of the summary in terms of capturing important n-gram phrases.\n",
    "\n",
    "2. ROUGE-L: This metric considers the longest common subsequence (LCS) between the generated summary and the reference summary. It calculates precision, recall, and F1-score based on the length of the LCS normalized by the lengths of the generated and reference summaries. ROUGE-L captures the fluency and coherence of the summary by rewarding longer common subsequences.\n",
    "\n",
    "3. ROUGE-S: This metric evaluates the skip-bigram (or skip-2) matches between the generated summary and the reference summary. It calculates precision, recall, and F1-score based on the number of skip-bigrams normalized by the average skip-bigrams count in the generated and reference summaries. ROUGE-S captures the ability of the summary to capture non-contiguous word associations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdf1b85-d469-4210-a632-e05def027575",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
