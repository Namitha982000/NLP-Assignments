{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da6a2db5-1e6c-4b77-86f9-a7c814096491",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd1df73-6da1-40d0-84a0-1d036e9ccd24",
   "metadata": {},
   "source": [
    "#### 1. Explain the basic architecture of RNN cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5e618f-64f4-4d50-95d6-5985fc20ad03",
   "metadata": {},
   "source": [
    "A Recurrent Neural Network (RNN) cell's fundamental structure is made up of three layers: an input layer, a hidden layer, and an output layer. A hidden state that stores the context and data from earlier steps is updated and maintained by the RNN cell as it processes sequential data.\n",
    "\n",
    "The fundamental design of an RNN cell is described in the following manner:\n",
    "\n",
    "1. Input Layer: The input layer receives the input sequence at each time step. In the case of text data, each time step corresponds to a word or a character in the sequence. The input is usually represented as a vector or embedding that captures the features of the input at that time step.\n",
    "\n",
    "2. Hidden Layer: The hidden layer is responsible for capturing and maintaining the sequential information over time. At each time step, the hidden state is updated based on the input at that time step and the previous hidden state. This allows the RNN to capture the context and dependencies between different elements of the sequence. The hidden state is typically represented as a vector of fixed size, and its dimensionality determines the memory capacity of the RNN.\n",
    "\n",
    "3. Output Layer: The output layer takes the hidden state at each time step and produces the corresponding output or prediction. The output can be a single value, a sequence of values, or a probability distribution over a set of classes, depending on the specific task the RNN is designed for.\n",
    "\n",
    "The update of the hidden state in an RNN cell is typically done using a combination of the input at the current time step and the previous hidden state. This update operation is often represented by a recurrent weight matrix that connects the hidden state to itself. This weight matrix captures the temporal dependencies and allows the RNN to propagate information over multiple time steps.\n",
    "\n",
    "One important characteristic of the RNN architecture is that it allows for the use of the same set of parameters (weights) across all time steps, enabling the model to process sequences of variable length.\n",
    "\n",
    "The basic RNN cell can be extended to various variants, such as Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU), which address the vanishing gradient problem and improve the ability to capture long-term dependencies in sequential data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2dcdda-1a63-40f2-a65d-b5b344a1cc55",
   "metadata": {},
   "source": [
    "#### 2. Explain Backpropagation through time (BPTT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0919459f-3b7c-423e-9776-39e133a82bff",
   "metadata": {},
   "source": [
    "Recurrent neural networks (RNNs) are trained using the backpropagation through time (BPTT) algorithm, which extends the backpropagation process to accommodate sequences or time-dependent data. RNNs, which include recurrent connections that enable information to be processed and propagated over time, have the challenge of propagating gradients across several time steps, and BPTT is specifically created to handle this issue.\n",
    "\n",
    "The BPTT algorithm is explained in detail below:\n",
    "\n",
    "1. Forward Pass: In the forward pass, the input sequence is fed into the RNN one time step at a time. The RNN processes each time step by updating its hidden state based on the input and the previous hidden state. The output at each time step is computed based on the current hidden state.\n",
    "\n",
    "2. Loss Calculation: After the forward pass, a loss function is computed to measure the discrepancy between the predicted output and the desired output at each time step. The loss is typically computed using a suitable loss function for the specific task, such as mean squared error (MSE) for regression or cross-entropy loss for classification.\n",
    "\n",
    "3. Backward Pass: In the backward pass, the gradients of the loss function with respect to the parameters of the RNN are computed. Starting from the final time step, the gradients are propagated backward through time, one time step at a time. At each time step, the gradients are computed based on the current gradient and the gradients from the subsequent time step.\n",
    "\n",
    "4. Gradient Update: Once the gradients are computed, the parameters of the RNN are updated using an optimization algorithm, such as stochastic gradient descent (SGD) or Adam. The gradients are used to update the weights and biases of the RNN, allowing the model to learn from the errors and improve its predictions.\n",
    "\n",
    "During training, the BPTT method basically unrolls the RNN throughout the whole length of the sequence, treating each time step independently. As a result, the gradients can be backpropagated throughout time, allowing the model to pick up knowledge from the entire sequence. It also presents difficulties like the disappearing or exploding gradient problem, in which gradients may vanish or increase exponentially over extended periods.\n",
    "\n",
    "Gradient clipping and specialised RNN variations like Long Short-Term Memory (LSTM) or Gated Recurrent Unit (GRU) are frequently used as mitigation strategies for these problems. These variations make use of additional mechanisms to regulate information flow and deal with the vanishing gradient issue, making it possible to train RNNs using BPTT more successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd16252c-ea6f-4382-9740-20f04604ddc1",
   "metadata": {},
   "source": [
    "#### 3. Explain Vanishing and exploding gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886e8478-1721-4322-834b-635542ead0f5",
   "metadata": {},
   "source": [
    "Deep neural networks (DNNs) can experience disappearing and expanding gradients during training, especially recurrent neural networks (RNNs), which feature recurrent connections and long-term dependencies.\n",
    "\n",
    "1. Vanishing Gradients: The vanishing gradient problem refers to the phenomenon where the gradients computed during backpropagation become extremely small as they are propagated from the output layer to the earlier layers of the network. This occurs when the gradients are successively multiplied by the weights in each layer during backpropagation, and if the weights are less than 1, the gradients can diminish exponentially.\n",
    "\n",
    "When the gradients become very small, it becomes challenging for the network to update the weights effectively. The layers closer to the input receive weak gradients, resulting in slow learning or even no learning at all. Consequently, these layers fail to capture complex dependencies or meaningful patterns in the data, limiting the overall performance of the network.\n",
    "\n",
    "2. Exploding Gradients: Conversely, the exploding gradient problem occurs when the gradients become extremely large during backpropagation. This can happen when the gradients are successively multiplied by weights greater than 1, leading to exponential growth. As a result, the weight updates become very large, causing unstable training and making it difficult for the model to converge.\n",
    "\n",
    "The exploding gradient problem can cause the network to diverge, leading to unstable predictions and failure to learn meaningful representations from the data.\n",
    "\n",
    "Both vanishing and exploding gradients can hinder the training of deep neural networks, including RNNs. These problems are particularly pronounced in deep architectures where the gradients need to be propagated through many layers or time steps. These issues are more prevalent in traditional RNNs due to the nature of their recurrent connections, which can amplify the impact of gradient propagation.\n",
    "\n",
    "To mitigate these problems, several techniques have been developed:\n",
    "\n",
    "1. Gradient Clipping: Gradient clipping is a technique that limits the magnitude of the gradients during backpropagation. By setting a threshold, the gradients are rescaled if they exceed the threshold, preventing them from growing too large or too small.\n",
    "\n",
    "2. Weight Initialization: Proper initialization of the weights can also help alleviate the vanishing or exploding gradient problem. Techniques like Xavier or He initialization can be used to initialize the weights in a way that maintains a suitable range of gradients during training.\n",
    "\n",
    "3. Nonlinear Activation Functions: The choice of activation functions can also impact gradient propagation. Using activation functions that do not saturate (e.g., ReLU, Leaky ReLU) can alleviate the vanishing gradient problem by allowing gradients to flow more effectively.\n",
    "\n",
    "4. Gradient Regularization: Regularization techniques like L1 or L2 regularization, dropout, or batch normalization can help stabilize gradient updates and improve the generalization of the model.\n",
    "\n",
    "5. Architectural Modifications: Using specialized RNN variants like Long Short-Term Memory (LSTM) or Gated Recurrent Unit (GRU) can address the vanishing gradient problem by incorporating gating mechanisms that regulate the flow of information and gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6669e2e0-6e6e-4ad8-bf7e-c04aef2a088c",
   "metadata": {},
   "source": [
    "#### 4. Explain Long short-term memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1035ed04-3728-440d-a472-20c4c85b0977",
   "metadata": {},
   "source": [
    "Recurrent neural network (RNN) architecture known as Long Short-Term Memory (LSTM) was created to overcome the vanishing gradient problem and capture long-term dependencies in sequential data. When processing and producing predictions based on data sequences like text, speech, time series, or video, LSTMs are especially successful.\n",
    "\n",
    "The insertion of memory cells, which enable the network to retain and access information over a wide time range, is the fundamental concept underpinning LSTM. Three essential parts—the input gate, the forget gate, and the output gate—help LSTMs do this. These gates manage the store and retrieval of memory data as well as the information flow within the LSTM unit.\n",
    "\n",
    "Here's a breakdown of the components and their functions in an LSTM unit:\n",
    "\n",
    "1. Memory Cell (C): The memory cell serves as the central element of an LSTM unit. It stores and updates information over time by incorporating three operations: input, forget, and output.\n",
    "\n",
    "2. Input Gate (i): The input gate determines the extent to which new information should be added to the memory cell. It takes input from the current input data and the previous hidden state (output of the previous time step) and applies a sigmoid activation function. This gate decides which values should be updated and creates a candidate update vector (g) using a hyperbolic tangent activation function.\n",
    "\n",
    "3. Forget Gate (f): The forget gate decides which information should be discarded from the memory cell. It takes input from the current input data and the previous hidden state and applies a sigmoid activation function. The output of this gate is multiplied element-wise with the previous cell state to selectively forget or retain information.\n",
    "\n",
    "4. Output Gate (o): The output gate determines the extent to which the information stored in the memory cell should be exposed to the network's output. It takes input from the current input data and the previous hidden state and applies a sigmoid activation function. The output of this gate is multiplied element-wise with the updated memory cell state (after applying the input and forget operations) to produce the current hidden state.\n",
    "\n",
    "5. Hidden State (h): The hidden state serves as the output of the LSTM unit and contains the summarized information from the input sequence. It is obtained by applying the output gate operation to the updated memory cell state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8f30d6-9b7e-4b7e-a0c2-083add83cfa3",
   "metadata": {},
   "source": [
    "#### 5. Explain Gated recurrent unit (GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccc0fe9-ebc3-49ef-a548-c47a891347b1",
   "metadata": {},
   "source": [
    "Similar to LSTM, the Gated Recurrent Unit (GRU) is a different type of recurrent neural network (RNN) architecture that was created to simulate long-term dependencies in sequential data and solve the vanishing gradient problem. By integrating the forget and input gates into a single update gate, GRUs streamline the LSTM architecture, resulting in a more streamlined and computationally effective model.\n",
    "\n",
    "The following elements make up the GRU unit:\n",
    "\n",
    "1. Update Gate (z): The update gate determines how much of the previous hidden state should be retained and how much of the new information should be integrated into the current hidden state. It takes input from the current input data and the previous hidden state and applies a sigmoid activation function. The output of this gate controls the flow of information from the previous hidden state to the current hidden state.\n",
    "\n",
    "2. Reset Gate (r): The reset gate decides how much of the previous hidden state should be ignored while computing the current hidden state. It takes input from the current input data and the previous hidden state and applies a sigmoid activation function. The output of this gate is used to reset the hidden state, allowing the model to focus on the relevant information in the input sequence.\n",
    "\n",
    "3. Current Hidden State (h): The current hidden state represents the memory or information accumulated from previous time steps. It is a combination of the previous hidden state modified by the update gate and a new candidate hidden state computed using the current input data and the reset gate. The candidate hidden state is computed by applying a hyperbolic tangent activation function to the concatenation of the current input and the element-wise product of the reset gate and the previous hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76776f15-b203-4f9a-9652-759d05132d60",
   "metadata": {},
   "source": [
    "#### 6. Explain Peephole LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621cb90f-6aba-4ab7-9cef-bccf5264c685",
   "metadata": {},
   "source": [
    "The Long Short-Term Memory (LSTM) architecture is expanded by the Peephole LSTM design, which adds peephole connections. As a result, the LSTM cell now has direct access to the cell state in addition to the input and hidden state. This new connection gives the LSTM cell more knowledge so it can decide more carefully while learning.\n",
    "\n",
    "The forget gate, input gate, and output gate, which manage the information flow through the cell, change the cell state in a normal LSTM cell. In contrast, a peephole LSTM directly connects the cell state to the gates through peephole connections. As a result, the gates have access to the current cell state and can utilise it to supplement their knowledge when making decisions.\n",
    "\n",
    "The forget gate, input gate, and output gate formulae are amended to include the peephole connections as follows:\n",
    "\n",
    "1. Forget Gate: The forget gate determines how much of the previous cell state should be forgotten. In a peephole LSTM, the forget gate takes into account both the previous hidden state and the previous cell state through peephole connections. The equation for the forget gate becomes:\n",
    "\n",
    "   f_t = sigmoid(W_f * x_t + U_f * h_(t-1) + V_f * c_(t-1) + b_f)\n",
    "\n",
    "2. Input Gate: The input gate controls how much of the new input should be stored in the cell state. Similar to the forget gate, the input gate in a peephole LSTM incorporates the previous hidden state and the previous cell state. The equation for the input gate becomes:\n",
    "\n",
    "   i_t = sigmoid(W_i * x_t + U_i * h_(t-1) + V_i * c_(t-1) + b_i)\n",
    "\n",
    "3. Cell State: The cell state is updated by combining the information from the input gate and the peephole connection from the forget gate. The equation for the updated cell state becomes:\n",
    "\n",
    "   c_t = f_t * c_(t-1) + i_t * tanh(W_c * x_t + U_c * h_(t-1) + b_c)\n",
    "\n",
    "4. Output Gate: The output gate determines how much of the current cell state should be outputted as the hidden state. In a peephole LSTM, the output gate includes a peephole connection from the current cell state. The equation for the output gate becomes:\n",
    "\n",
    "   o_t = sigmoid(W_o * x_t + U_o * h_(t-1) + V_o * c_t + b_o)\n",
    "\n",
    "5. Hidden State: The hidden state is computed by applying the output gate to the cell state. The equation for the hidden state becomes:\n",
    "\n",
    "   h_t = o_t * tanh(c_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32419bf-64af-48cf-952e-09415620b2a3",
   "metadata": {},
   "source": [
    "#### 7. Bidirectional RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a491927a-18c2-4553-9c3d-e20e419f6920",
   "metadata": {},
   "source": [
    "Recurrent neural networks (RNNs) that process input sequences both forward and backward are known as bidirectional RNNs. The hidden state in a typical RNN depends solely on past time steps at each time step. The hidden state at each time step in a bidirectional RNN, however, is affected by both the previous and subsequent time steps.\n",
    "\n",
    "The architecture of a bidirectional RNN consists of two separate RNNs: one processing the input sequence in the forward direction (from the beginning to the end) and the other processing the input sequence in the backward direction (from the end to the beginning). Each RNN has its own set of parameters and hidden states. The hidden states from both directions are then combined or concatenated to form the final representation.\n",
    "\n",
    "The forward RNN computes the hidden states in the standard manner, starting from the first input and moving forward through time. The backward RNN, on the other hand, processes the input sequence in reverse order, starting from the last input and moving backward through time. At each time step, the hidden state of the backward RNN depends on both the current input and the hidden state from the next time step.\n",
    "\n",
    "The main advantage of using bidirectional RNNs is that they capture both past and future context for each time step. This can be particularly useful in tasks where the current output depends on the entire input sequence, such as sequence labeling, sentiment analysis, machine translation, and speech recognition. By incorporating information from both directions, bidirectional RNNs can capture more comprehensive and richer representations of the input sequence, improving the model's ability to understand and predict sequence patterns.\n",
    "\n",
    "Bidirectional RNNs need the complete input sequence to be available up front, which may not be possible in cases involving streaming or real-time data. This is an essential point to keep in mind. Due to the requirement to analyse the sequence in both directions, bidirectional RNNs can frequently demand more memory and have higher computational complexity than regular RNNs. However, they are a useful tool for many sequence-related tasks due to their improved modelling capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be6b5b4-cddf-44b7-9bd5-66010605c759",
   "metadata": {},
   "source": [
    "#### 8. Explain the gates of LSTM with equations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63eb9ac-83f6-4add-a63e-e8fa405539ae",
   "metadata": {},
   "source": [
    "LSTM (Long Short-Term Memory) networks use gates to control the flow of information within the network. The gates are responsible for selectively allowing or blocking information from passing through, enabling the network to learn long-term dependencies. LSTM networks typically have three main gates: the input gate, the forget gate, and the output gate. Here's an explanation of each gate along with the corresponding equations:\n",
    "\n",
    "1. Input Gate (i):\n",
    "The input gate determines how much new information should be stored in the cell state. It takes the current input and the previous hidden state as inputs and outputs a value between 0 and 1 for each element in the cell state vector. The equations for the input gate are:\n",
    "\n",
    "   i_t = σ(W_i * [h_(t-1), x_t] + b_i)\n",
    "\n",
    "   Where:\n",
    "   - i_t: input gate activation at time step t\n",
    "   - σ: sigmoid activation function\n",
    "   - W_i: weight matrix for the input gate\n",
    "   - h_(t-1): previous hidden state\n",
    "   - x_t: current input\n",
    "   - b_i: bias vector for the input gate\n",
    "\n",
    "2. Forget Gate (f):\n",
    "The forget gate determines which information from the previous cell state should be discarded. It takes the current input and the previous hidden state as inputs and outputs a value between 0 and 1 for each element in the cell state vector. The equations for the forget gate are:\n",
    "\n",
    "   f_t = σ(W_f * [h_(t-1), x_t] + b_f)\n",
    "\n",
    "   Where:\n",
    "   - f_t: forget gate activation at time step t\n",
    "   - σ: sigmoid activation function\n",
    "   - W_f: weight matrix for the forget gate\n",
    "   - h_(t-1): previous hidden state\n",
    "   - x_t: current input\n",
    "   - b_f: bias vector for the forget gate\n",
    "\n",
    "3. Output Gate (o):\n",
    "The output gate determines how much information from the cell state should be exposed to the current hidden state and the output. It takes the current input and the previous hidden state as inputs and outputs a value between 0 and 1 for each element in the cell state vector. The equations for the output gate are:\n",
    "\n",
    "   o_t = σ(W_o * [h_(t-1), x_t] + b_o)\n",
    "\n",
    "   Where:\n",
    "   - o_t: output gate activation at time step t\n",
    "   - σ: sigmoid activation function\n",
    "   - W_o: weight matrix for the output gate\n",
    "   - h_(t-1): previous hidden state\n",
    "   - x_t: current input\n",
    "   - b_o: bias vector for the output gate\n",
    "\n",
    "These gate activations control the flow of information through the LSTM cell. The cell state is updated using the following equations:\n",
    "\n",
    "   C_t = f_t * C_(t-1) + i_t * tanh(W_C * [h_(t-1), x_t] + b_C)\n",
    "\n",
    "   Where:\n",
    "   - C_t: updated cell state at time step t\n",
    "   - f_t: forget gate activation at time step t\n",
    "   - C_(t-1): previous cell state\n",
    "   - i_t: input gate activation at time step t\n",
    "   - tanh: hyperbolic tangent activation function\n",
    "   - W_C: weight matrix for the cell state\n",
    "   - h_(t-1): previous hidden state\n",
    "   - x_t: current input\n",
    "   - b_C: bias vector for the cell state\n",
    "\n",
    "The updated cell state is then used to compute the current hidden state:\n",
    "\n",
    "   h_t = o_t * tanh(C_t)\n",
    "\n",
    "   Where:\n",
    "   - h_t: current hidden state\n",
    "   - o_t: output gate activation at time step t\n",
    "   - tanh: hyper\n",
    "\n",
    "bolic tangent activation function\n",
    "   - C_t: updated cell state at time step t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33bc8d9-eb5a-4940-8bc8-6096bb392930",
   "metadata": {},
   "source": [
    "#### 9. Explain BiLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b489a8-213e-44af-b10a-d724cb44242d",
   "metadata": {},
   "source": [
    "An addition to the LSTM architecture called BiLSTM (Bidirectional Long Short-Term Memory) includes knowledge from both the past and the future. The BiLSTM simultaneously processes the input sequence in both the forward and backward directions, unlike the regular LSTM, which only takes into account the previous context.\n",
    "\n",
    "The BiLSTM is made up of two LSTM layers, one of which processes the input sequence forwardly (from beginning to finish) and the other of which processes it backwardly (from end to beginning). There are distinct hidden states and cell states for every LSTM layer.\n",
    "\n",
    "Here is a detailed description of how BiLSTM operates:\n",
    "\n",
    "1. Input Encoding:\n",
    "The input sequence is encoded into a sequence of word embeddings or other input representations. Each word in the sequence is transformed into a fixed-size vector representation.\n",
    "\n",
    "2. Forward LSTM:\n",
    "The forward LSTM layer processes the input sequence in the forward direction. At each time step, the forward LSTM takes the current input and the previous hidden state as inputs and computes the new hidden state and cell state using the LSTM equations. The hidden state captures information from the past context.\n",
    "\n",
    "3. Backward LSTM:\n",
    "The backward LSTM layer processes the input sequence in the backward direction. At each time step, the backward LSTM takes the current input and the previous hidden state as inputs and computes the new hidden state and cell state using the LSTM equations. The hidden state captures information from the future context.\n",
    "\n",
    "4. Concatenation:\n",
    "After both forward and backward LSTM layers have processed the input sequence, the hidden states from both directions are concatenated at each time step. This combines the information from the past and future contexts into a single representation.\n",
    "\n",
    "5. Output:\n",
    "The concatenated hidden states can be further processed by additional layers or directly used for the desired task, such as sequence classification or sequence-to-sequence translation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d00a982-4654-41de-a664-764e965fd7ab",
   "metadata": {},
   "source": [
    "#### 10. Explain BiGRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb83d001-6e6b-4120-9074-bbc36d70fcbb",
   "metadata": {},
   "source": [
    "The GRU (Gated Recurrent Unit) architecture's BiGRU (Bidirectional Gated Recurrent Unit) variation simultaneously processes the input sequence in both the forward and backward directions. Similar to BiLSTM, BiGRU adds past and future context information, making it possible to grasp the input sequence more thoroughly.\n",
    "\n",
    "Here is a detailed description of how BiGRU operates:\n",
    "\n",
    "1. Input Encoding:\n",
    "The input sequence is encoded into a sequence of word embeddings or other input representations. Each word in the sequence is transformed into a fixed-size vector representation.\n",
    "\n",
    "2. Forward GRU:\n",
    "The forward GRU layer processes the input sequence in the forward direction. At each time step, the forward GRU takes the current input and the previous hidden state as inputs and computes the new hidden state using the GRU equations. The hidden state captures information from the past context.\n",
    "\n",
    "3. Backward GRU:\n",
    "The backward GRU layer processes the input sequence in the backward direction. At each time step, the backward GRU takes the current input and the previous hidden state as inputs and computes the new hidden state using the GRU equations. The hidden state captures information from the future context.\n",
    "\n",
    "4. Concatenation:\n",
    "After both forward and backward GRU layers have processed the input sequence, the hidden states from both directions are concatenated at each time step. This combines the information from the past and future contexts into a single representation.\n",
    "\n",
    "5. Output:\n",
    "The concatenated hidden states can be further processed by additional layers or directly used for the desired task, such as sequence classification or sequence-to-sequence translation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93d0e35-3663-4a55-a15b-167d81427752",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
