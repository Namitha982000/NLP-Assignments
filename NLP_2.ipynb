{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d161f4ca-5be2-4db2-99fc-e00bd49b0f5e",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9422bca2-c371-4f8e-9f48-1c311c41e8c4",
   "metadata": {},
   "source": [
    "#### 1. What are Corpora?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4332eba-6855-40e0-bde2-fc5f827ff2c1",
   "metadata": {},
   "source": [
    "Large collections of texts or linguistic data are referred to as corpora in the area of natural language processing (NLP). An organised dataset of written or spoken language is referred to as a corpus (plural: corpora), and it is frequently used for linguistic analysis, machine learning, and other NLP applications.\n",
    "\n",
    "Depending on the precise goals of the research or application, corpora might vary in size, scope, and composition. They can be assembled from a variety of sources, including books, articles, websites, posts on social media, spoken conversation transcripts, and more.\n",
    "\n",
    "Corpora serve as valuable resources for studying language patterns, understanding linguistic phenomena, and training NLP models. They provide researchers and practitioners with a representative sample of language usage and allow for the extraction of meaningful insights and statistics.\n",
    "\n",
    "Corpora can be categorized based on different criteria, such as genre (e.g., news corpus, scientific corpus, literary corpus), language (e.g., English corpus, Spanish corpus), or specific domains (e.g., medical corpus, legal corpus). Each corpus may have specific characteristics and properties that make it suitable for particular analysis or application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8b47e4-abbd-4c82-bb36-52dc07c72e6c",
   "metadata": {},
   "source": [
    "#### 2. What are Tokens?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfc3197-1c6d-4fd5-b304-6dbd71b11a55",
   "metadata": {},
   "source": [
    "Tokens are the discrete components or pieces that make up a text or document when natural language processing (NLP) is used. The level of granularity selected for tokenization determines whether these units are words, characters, or subwords.\n",
    "\n",
    "Tokenization is the process of disassembling a text into smaller pieces that are then handled as distinct entities for processing or analysis. Tokenization is a crucial preprocessing step in NLP jobs since it gives the model the ability to comprehend and interact with specific text parts.\n",
    "\n",
    "Here are a few examples of tokens based on different tokenization approaches:\n",
    "\n",
    "1. Word Tokens: In word-level tokenization, each word in the text is considered as a separate token. For example, the sentence \"I love natural language processing\" would be tokenized into individual words: [\"I\", \"love\", \"natural\", \"language\", \"processing\"].\n",
    "\n",
    "2. Character Tokens: In character-level tokenization, each character in the text is treated as a separate token. For example, the word \"hello\" would be tokenized into individual characters: [\"h\", \"e\", \"l\", \"l\", \"o\"].\n",
    "\n",
    "3. Subword Tokens: Subword-level tokenization involves breaking down words into smaller meaningful units, such as prefixes, suffixes, or root words. This approach is useful for dealing with out-of-vocabulary words and improving the coverage of rare or complex words. For example, the word \"unhappiness\" might be tokenized into [\"un\", \"happiness\"].\n",
    "\n",
    "For a variety of NLP activities, tokens act as the fundamental building elements. The tokens from the tokenized text can then be utilised for tasks like text categorization, sentiment analysis, machine translation, and more. By functioning at the level of individual tokens, tokenization enables NLP models to comprehend the text's structure and meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436c76c2-cc13-431e-83c5-1d43067590a0",
   "metadata": {},
   "source": [
    "#### 3. What are Unigrams, Bigrams, Trigrams?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6f9a9c-7cb4-4e20-a6ef-033e8c80311e",
   "metadata": {},
   "source": [
    "In the field of natural language processing (NLP), words like \"unigrams,\" \"bigrams,\" and \"trigrams\" are used to describe various n-gram model levels. Contiguous sequences of n elements (usually words) make up an n-gram, which is taken from a larger text corpus. An explanation of each follows:\n",
    "\n",
    "1. Unigrams: Unigrams are individual words considered as isolated units. In this case, the text is tokenized into individual words, and each word is treated as a separate unigram. For example, the sentence \"I love natural language processing\" would be represented by the unigrams: [\"I\", \"love\", \"natural\", \"language\", \"processing\"]. Unigrams provide basic word-level information and can be useful in tasks like text classification or basic frequency analysis.\n",
    "\n",
    "2. Bigrams: Bigrams consist of two adjacent words occurring together in the text. The sentence \"I love natural language processing\" would be represented by the bigrams: [\"I love\", \"love natural\", \"natural language\", \"language processing\"]. Bigrams capture some local contextual information and can be used to understand word co-occurrence patterns, language models, or certain types of syntactic relationships.\n",
    "\n",
    "3. Trigrams: Trigrams are sequences of three consecutive words occurring together in the text. Taking the same sentence as an example, the trigrams would be: [\"I love natural\", \"love natural language\", \"natural language processing\"]. Trigrams provide a slightly wider context compared to bigrams and can capture more complex relationships between words.\n",
    "\n",
    "NLP tasks like language modelling, machine translation, information retrieval, and more require N-grams, including unigrams, bigrams, and trigrams. N-gram models can identify regional patterns and dependencies in the text by taking into account the relationship between neighbouring words or tokens. The work at hand, as well as the needed level of context and information, will determine which n-gram level is most suited."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b2db08-1bc2-4ca9-97ff-cdef4887db60",
   "metadata": {},
   "source": [
    "#### 4. How to generate n-grams from text?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ffaea7-a3b6-43c1-8ec2-a23345d28a56",
   "metadata": {},
   "source": [
    "To generate n-grams from text, you can follow these steps:\n",
    "\n",
    "1. Tokenize the Text: Split the text into individual words or tokens. This can be done using libraries or functions available in programming languages like Python. For example, you can use the `split()` function in Python to split the text into words.\n",
    "\n",
    "2. Preprocess the Text: Apply any necessary preprocessing steps to the text, such as converting all words to lowercase, removing punctuation, or removing stopwords (commonly used words like \"the\", \"is\", etc.) if needed. This step helps in generating cleaner and more meaningful n-grams.\n",
    "\n",
    "3. Generate N-Grams: Iterate over the tokenized text and create n-grams by considering consecutive tokens. For example, to generate bigrams (n=2), you would pair each word with its adjacent word, and for trigrams (n=3), you would consider three consecutive words together.\n",
    "\n",
    "4. Store the N-Grams: Collect the generated n-grams in a suitable data structure, such as a list or a dictionary, depending on your requirements. You can also choose to store the n-grams as strings or tuples, depending on whether you need to preserve the ordering of the tokens or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee280f2-7ae5-4821-ae1f-e6fd3396cf72",
   "metadata": {},
   "source": [
    "#### 5. Explain Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b275c6c9-b5a1-471d-9647-990edb526c82",
   "metadata": {},
   "source": [
    "Lemmatization is a method for reducing words to their lemma, or basic or root form, in natural language processing. The lemma stands for a word's canonical, dictionary, or base form, which aids in normalising and condensing words to their most basic form.\n",
    "\n",
    "Lemmatization is the process of reducing words to their most basic form while taking into account the morphological examination of words. Lemmatization takes into account the context and grammatical characteristics of words to produce correct and meaningful lemmas, as opposed to stemming, which merely eliminates prefixes or suffixes to approximate the root form.\n",
    "\n",
    "Here's an example to illustrate lemmatization:\n",
    "\n",
    "Input Sentence: \"The cats are running and playing in the garden.\"\n",
    "\n",
    "Lemmatized Output: \"The cat be run and play in the garden.\"\n",
    "\n",
    "In this example, the word \"cats\" is lemmatized to \"cat\" because \"cat\" is the base form or lemma of the word. Similarly, \"running\" is lemmatized to \"run\" and \"playing\" is lemmatized to \"play\". Lemmatization ensures that words with the same meaning or similar context are reduced to a common base form, making it easier to analyze and process the text.\n",
    "\n",
    "Lemmatization is often performed using linguistic resources, such as dictionaries or word corpora, which contain information about the different forms and variants of words. There are also various libraries and tools available in different programming languages, such as NLTK (Natural Language Toolkit) in Python or spaCy, that provide lemmatization functionality. These libraries use language-specific rules and algorithms to accurately lemmatize words based on their part of speech and context within a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9a5e91-8be0-48e1-9500-6d2ac194239c",
   "metadata": {},
   "source": [
    "#### 6. Explain Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeac9b3-768c-45ef-91a5-29ca1e7167d6",
   "metadata": {},
   "source": [
    "The goal of the natural language processing (NLP) text normalisation approach known as stemming is to break down words into their stems, or base or root forms. The stem expresses the fundamental meaning of the word even though it may not always be a meaningful word by itself. In order to get at a word's stem, stemming includes removing its prefixes, suffixes, and inflectional endings.\n",
    "\n",
    "In applications like information retrieval, text mining, and sentiment analysis when word variants are not essential for analysis, stemming reduces several forms of a word to a common form. By treating alternative spellings of a word as the same, stemming helps to condense the vocabulary and boost computational speed.\n",
    "\n",
    "Here is an illustration of stemming:\n",
    "\n",
    "Input Word: \"Running\"\n",
    "\n",
    "Stemmed Output: \"Run\"\n",
    "\n",
    "In this example, the word \"running\" is stemmed to \"run\" by removing the suffix \"-ning\". Stemming algorithms use different rules and techniques to perform the stemming process. Some popular stemming algorithms include the Porter stemming algorithm, Snowball stemming algorithm, and Lancaster stemming algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177aec45-3615-41ff-ad73-d0fc4385510a",
   "metadata": {},
   "source": [
    "#### 7. Explain Part-of-speech (POS) tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a03a7bb-7471-4330-8087-781b71a77e20",
   "metadata": {},
   "source": [
    "Part-of-speech (POS) tagging, often referred to as grammatical tagging, is a technique used in natural language processing (NLP) that includes assigning a part of speech to each word in a document. The syntactic category of a word, such as a noun, verb, adjective, adverb, pronoun, preposition, conjunction, etc., is referred to as the part of speech. Understanding a sentence's grammatical structure and extracting useful information from text depend on POS tagging.\n",
    "\n",
    "In order to identify the proper part of speech for a word, POS tagging examines its context and adjacent words. Usually, rule-based systems, machine learning techniques, or a combination of both are used to do this.\n",
    "\n",
    "Machine learning-based POS tagging methods use pre-labeled training data, known as annotated corpora, to learn patterns and statistical models for assigning the correct part of speech tags to words. These models can be based on hidden Markov models (HMM), maximum entropy models (MaxEnt), or deep learning approaches like recurrent neural networks (RNN) or transformers.\n",
    "\n",
    "Rule-based POS tagging systems rely on linguistic rules and heuristics to assign POS tags. These rules consider factors such as word suffixes, prefixes, capitalization, and word context to make the tagging decisions. Rule-based systems are often manually created by linguists or NLP experts.\n",
    "\n",
    "Text parsing, information extraction, sentiment analysis, and machine translation are just a few of the NLP tasks that benefit from POS tagging. The grammatical organisation and syntactic linkages inside a sentence are clarified, allowing for more precise analysis and interpretation of text data.\n",
    "\n",
    "NLP libraries and frameworks, such as NLTK (Natural Language Toolkit) in Python, which offers pre-trained taggers and tools for doing POS tagging on text data, support POS tagging to a large extent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1df6da8-b62b-45f1-8881-545443576b77",
   "metadata": {},
   "source": [
    "#### 8. Explain Chunking or shallow parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef5e4a8-40f4-4caf-95a5-7cc9da3afa2a",
   "metadata": {},
   "source": [
    "A natural language processing (NLP) technique called chunking, commonly referred to as shallow parsing, groups words together into understandable chunks depending on their syntactic structure. It seeks to recognise and extract words or phrases that reflect higher-level meaning units, such as prepositional phrases, verb tenses, and noun phrases.\n",
    "\n",
    "Chunking frequently comes after part-of-speech (POS) tagging, which labels each word in a phrase with the appropriate part of speech. A chunker uses the POS tags as input and pattern matching or machine learning algorithms to find and group words that make sense together.\n",
    "\n",
    "The most common type of chunking is noun phrase (NP) chunking, which identifies and extracts noun phrases from a sentence. A noun phrase is a group of words centered around a noun and includes any modifiers or determiners that accompany the noun. For example, in the sentence \"The big brown dog chased the squirrel,\" the noun phrase chunks would be \"The big brown dog\" and \"the squirrel.\"\n",
    "\n",
    "Another type of chunking is verb phrase (VP) chunking, which focuses on identifying and extracting verb phrases from a sentence. A verb phrase consists of a verb along with its associated modifiers, objects, or complements. For example, in the sentence \"She quickly read the book,\" the verb phrase chunk would be \"quickly read the book.\"\n",
    "\n",
    "Chunking can be carried either via machine learning or rule-based methods. Rule-based chunkers match and extract chunks based on established rules using patterns or regular expressions. Annotated training data are used by machine learning-based chunkers to discover patterns and statistical models for chunk identification. These models could be built using algorithms like conditional random fields (CRF), maximum entropy models (MaxEnt), or hidden Markov models (HMM).\n",
    "\n",
    "Chunking is helpful for several NLP applications, including question answering, text summarization, and information extraction. It aids in identifying and removing textual components that are significant and add to the overall comprehension of a sentence or document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ca14c2-d4b9-4950-94d4-faef1edac86b",
   "metadata": {},
   "source": [
    "#### 9. Explain Noun Phrase (NP) chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23c8c51-25d2-4f5e-b689-ad3ad6959882",
   "metadata": {},
   "source": [
    "Natural language processing (NLP) has a specific sort of chunking called noun phrase (NP) chunking that focuses on locating and extracting noun phrases from sentences. Any modifiers or determiners that go with the noun are included in a noun phrase, which is a set of words centred around the noun.\n",
    "\n",
    "Identifying and grouping the words that make up coherent noun phrases is the aim of NP chunking, which enables a more insightful study of the sentence structure. NP chunking is frequently done in conjunction with activities like information extraction or syntactic parsing.\n",
    "\n",
    "Here is an illustration of NP chunking:\n",
    "\n",
    "Original Sentence: \"The big brown dog chased the squirrel.\"\n",
    "\n",
    "NP Chunked Result: \"The big brown dog\", \"the squirrel\"\n",
    "\n",
    "In the example above, the noun phrase chunks identified are \"The big brown dog\" and \"the squirrel\". The chunks consist of the determiners \"The\" and \"the\", along with the modifiers \"big\" and \"brown\", followed by the nouns \"dog\" and \"squirrel\".\n",
    "\n",
    "NP chunking can be performed using rule-based approaches or machine learning techniques. Rule-based approaches utilize patterns or regular expressions to match and extract noun phrases based on predefined rules. For example, a simple rule may state that a noun phrase consists of an optional determiner, followed by one or more adjectives, and finally a noun.\n",
    "\n",
    "Annotated training data are used in machine learning-based systems to identify noun phrases by learning patterns and statistical models. These models could be built using algorithms like conditional random fields (CRF), maximum entropy models (MaxEnt), or hidden Markov models (HMM).\n",
    "\n",
    "NLP applications such as information extraction, text summarization, and question answering all benefit from NP chunking. It aids in recognising and extracting textual meaning units centred on nouns, enabling more thorough study and comprehension of the structure and substance of the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfc020f-1464-4fe6-b1a0-fe844b912caa",
   "metadata": {},
   "source": [
    "#### 10. Explain Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1ca134-5781-4053-8687-56549c97467b",
   "metadata": {},
   "source": [
    "The work of named entity recognition (NER), which falls under the purview of natural language processing (NLP), entails locating and categorising named entities in text according to predetermined standards. Named entities are particular named objects, such as individuals, groups, places, times, amounts, and other kinds of named entities.\n",
    "\n",
    "The objective of NER is to extract and label these named entities from text, allowing for a deeper comprehension of the data present therein and facilitating a variety of downstream applications like information retrieval, question answering, sentiment analysis, and more.\n",
    "\n",
    "The following steps are commonly involved in NER:\n",
    "\n",
    "1. Tokenization: The input text is divided into individual tokens, such as words or subword units.\n",
    "\n",
    "2. Part-of-Speech (POS) Tagging: Each token is assigned a part-of-speech tag, which helps identify the grammatical role of the word in the sentence.\n",
    "\n",
    "3. Named Entity Recognition: In this step, the system analyzes the tokens and determines whether they represent named entities. The recognized entities are classified into predefined categories like person, organization, location, date, etc.\n",
    "\n",
    "4. Entity Classification: Once the entities are recognized, they are further classified into specific subcategories or types. For example, a recognized entity may be classified as a person's name, company name, or geographical location.\n",
    "\n",
    "NER can be approached using different techniques, including rule-based methods, statistical models, and machine learning approaches. Rule-based methods rely on predefined patterns or rules to identify named entities based on specific linguistic patterns or syntactic structures. Statistical models and machine learning approaches leverage annotated training data to learn patterns and statistical models for recognizing and classifying named entities.\n",
    "\n",
    "NER systems are trained on labeled datasets where human annotators manually mark the named entities in the text. Commonly used machine learning algorithms for NER include conditional random fields (CRF), support vector machines (SVM), and deep learning-based approaches like recurrent neural networks (RNNs) and transformers.\n",
    "\n",
    "The output of NER is typically a sequence of tokens labeled with their corresponding named entity categories or tags. For example:\n",
    "\n",
    "Input Sentence: \"Apple Inc. is planning to open a new store in New York.\"\n",
    "\n",
    "NER Output: \"Apple Inc.\" (Organization), \"New York\" (Location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521538c1-be76-4fc4-b02f-cf1810f98dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
