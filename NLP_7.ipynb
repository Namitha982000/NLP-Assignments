{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2382ae2-5bdc-49ca-bba1-b2a07821255e",
   "metadata": {},
   "source": [
    "# Assignment 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd5f3c8-248e-410b-9106-bfa1ec2fa6ef",
   "metadata": {},
   "source": [
    "#### 1. Explain the architecture of BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60e0762-c4a4-4ecc-b8a2-c20774c649fb",
   "metadata": {},
   "source": [
    "1. Transformer Encoder: BERT is built upon a stack of Transformer encoder layers. Each encoder layer consists of a multi-head self-attention mechanism and a feed-forward neural network. The self-attention mechanism allows the model to capture contextual dependencies by attending to different parts of the input sequence. The feed-forward neural network processes the output of the self-attention layer to generate the final representations.\n",
    "\n",
    "2. Pre-training and Fine-tuning: BERT is pre-trained on a large corpus of text data using two unsupervised tasks: masked language modeling (MLM) and next sentence prediction (NSP). During pre-training, BERT learns to predict missing words in sentences (MLM) and to determine whether two sentences are consecutive or not (NSP). After pre-training, BERT's weights are fine-tuned on specific downstream tasks, such as text classification, named entity recognition, question answering, etc.\n",
    "\n",
    "3. Tokenization: BERT tokenizes the input text into subword units using WordPiece tokenization. This allows BERT to handle out-of-vocabulary (OOV) words and capture morphological variations. The input text is tokenized into a sequence of subword tokens, and special tokens like [CLS] (classification) and [SEP] (separator) are added to mark the beginning and end of sentences.\n",
    "\n",
    "4. Positional Encoding: BERT incorporates positional encoding to encode the positional information of words in the input sequence. This helps the model distinguish the order of words and capture the context effectively.\n",
    "\n",
    "5. Pre-training Objectives: BERT's pre-training objectives, MLM and NSP, enable it to learn a deep bidirectional representation of the input text. The MLM objective involves randomly masking some of the input tokens and predicting them based on the surrounding context. The NSP objective involves predicting whether two sentences appear consecutively in the original text.\n",
    "\n",
    "6. Fine-tuning and Task-Specific Layers: After pre-training, BERT's weights are fine-tuned on specific downstream tasks. Task-specific layers are added on top of the pre-trained BERT model to adapt it to the specific task requirements. These additional layers may include pooling, classification, or sequence labeling layers, depending on the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe45de71-c425-4ab6-aac3-9accfa73ec96",
   "metadata": {},
   "source": [
    "#### 2. Explain Masked Language Modeling (MLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0274f0d1-0c0f-4ed9-870c-5fff914a330c",
   "metadata": {},
   "source": [
    "To learn contextualised representations of words within a sentence, models like BERT (Bidirectional Encoder Representations from Transformers) use the pre-training aim known as \"masked language modelling\" (MLM). Predicting the hidden or missing words in a given sentence is the goal of MLM.\n",
    "\n",
    "A predetermined percentage of the input tokens in each sentence are randomly chosen and changed to a unique [MASK] token during the pre-training phase. Then, using the surrounding context, the model is trained to forecast the original masked tokens.\n",
    "\n",
    "The MLM goal has two functions:\n",
    "\n",
    "1. Bidirectional Context: By masking some tokens and predicting them based on the context, MLM allows the model to incorporate bidirectional information. It enables the model to understand the dependencies between words in both directions (before and after the masked token), capturing a deeper understanding of the context.\n",
    "\n",
    "2. Masked Token Representation: When predicting the masked tokens, the model generates representations that encapsulate the information of the surrounding words. This encourages the model to learn contextualized representations that capture the semantic and syntactic properties of the masked words.\n",
    "\n",
    "The model determines the probabilities of candidate words for each masked place and contrasts them to the original tokens in order to train the MLM aim. The model is tuned to reduce the discrepancy between the ground truth tokens and the projected probability.\n",
    "\n",
    "Due to the lack of explicit annotations, the MLM objective is an unsupervised learning problem. Models like BERT can acquire complex contextual representations that capture linguistic nuances and can be customised for a variety of downstream tasks including text classification, named entity recognition, question answering, and more by training on a large corpus of text data with MLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc08423-8874-495a-985d-11dd042cb392",
   "metadata": {},
   "source": [
    "#### 3. Explain Next Sentence Prediction (NSP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3ba7db-1c88-499e-bed0-4e9750f5fe85",
   "metadata": {},
   "source": [
    "Another pre-training goal employed in models like BERT (Bidirectional Encoder Representations from Transformers) to learn contextualised representations of sentences is next sentence prediction (NSP). The goal of NSP is to determine if two sentences are randomly sampled or whether they appear sequentially in the original text.\n",
    "\n",
    "The model is fed pairs of sentences during pre-training. For each pair, there is a 50% probability that the second sentence is drawn at random from the corpus and a 50% chance that it follows the first sentence in the original text. Predicting whether or not the second sentence is the actual successive sentence is the task of the model.\n",
    "\n",
    "The NSP objective serves to enhance the model's understanding of sentence-level relationships and discourse coherence. It helps the model capture the dependencies and semantic connections between consecutive sentences, which is crucial for tasks such as question answering, natural language inference, and text generation.\n",
    "\n",
    "The model uses its transformer-based architecture to encode both the first and second sentences in order to train the NSP aim. A classifier is then applied to the encoded representations to determine if the second sentence actually is a consecutive sentence or not.\n",
    "\n",
    "The model gains the ability to recognise the connections between sentences and the contextual information required to assess the coherence and continuity of the text through training on the NSP aim. As a result, the model is able to produce representations that are more contextually meaningful and performs better on tasks that require knowledge of sentence-level links down the road."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bf01c9-2e47-4e3e-848f-dbbb900d4c70",
   "metadata": {},
   "source": [
    "#### 4. What is Matthews evaluation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf0ea2b-2216-4367-9197-c7dd885b5294",
   "metadata": {},
   "source": [
    "Matthews evaluation, also known as the Matthews correlation coefficient (MCC), is a measure of the quality of binary (two-class) classification models. It takes into account true positives, true negatives, false positives, and false negatives to provide an overall assessment of the model's performance.\n",
    "\n",
    "The Matthews correlation coefficient is calculated using the following formula:\n",
    "\n",
    "MCC = (TP * TN - FP * FN) / sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))\n",
    "\n",
    "where:\n",
    "- TP (True Positives) is the number of instances correctly classified as positive.\n",
    "- TN (True Negatives) is the number of instances correctly classified as negative.\n",
    "- FP (False Positives) is the number of instances incorrectly classified as positive.\n",
    "- FN (False Negatives) is the number of instances incorrectly classified as negative.\n",
    "\n",
    "The MCC ranges from -1 to +1, where +1 indicates a perfect classification, 0 indicates a random classification, and -1 indicates a completely opposite classification.\n",
    "\n",
    "The MCC is particularly useful when dealing with imbalanced datasets, as it considers both the positive and negative classes and is not affected by class distribution. It provides a more reliable measure of model performance compared to accuracy, especially when the classes are unevenly distributed.\n",
    "\n",
    "The Matthews correlation coefficient is commonly used in various fields, including machine learning, bioinformatics, and information retrieval, to assess the performance of binary classification models and compare different models or algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb33c5b-e312-4d29-95b0-a709ae36af75",
   "metadata": {},
   "source": [
    "#### 5. What is Matthews Correlation Coefficient (MCC)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcae3bc3-a37c-472c-932b-85f2ca000b2b",
   "metadata": {},
   "source": [
    "The Matthews Correlation Coefficient (MCC) is a measure of the quality of binary (two-class) classification models. It takes into account true positives, true negatives, false positives, and false negatives to provide an overall assessment of the model's performance.\n",
    "\n",
    "The MCC is calculated using the following formula:\n",
    "\n",
    "MCC = (TP * TN - FP * FN) / sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))\n",
    "\n",
    "where:\n",
    "- TP (True Positives) is the number of instances correctly classified as positive.\n",
    "- TN (True Negatives) is the number of instances correctly classified as negative.\n",
    "- FP (False Positives) is the number of instances incorrectly classified as positive.\n",
    "- FN (False Negatives) is the number of instances incorrectly classified as negative.\n",
    "\n",
    "The MCC ranges from -1 to +1, where +1 indicates a perfect classification, 0 indicates a random classification, and -1 indicates a completely opposite classification.\n",
    "\n",
    "The MCC is a balanced measure that takes into account the distribution of the classes and provides a reliable evaluation of model performance, especially when dealing with imbalanced datasets. It is commonly used in machine learning, bioinformatics, and other fields where binary classification models are employed.\n",
    "\n",
    "The MCC is particularly useful when the classes are unevenly distributed or when there is a high imbalance between the number of true positives and true negatives. It is more robust than accuracy as it considers all four classification outcomes, making it a valuable metric for assessing and comparing binary classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7f71fe-708f-4888-8526-9336d6c07cec",
   "metadata": {},
   "source": [
    "#### 6. Explain Semantic Role Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6945ad1d-94f2-4a57-9073-f0403b6dc3c7",
   "metadata": {},
   "source": [
    "Natural language processing (NLP) job known as semantic role labelling (SRL) entails determining and categorising the functions that words or phrases in a sentence perform in relation to the main verb or predicate. By tying each word in a sentence to a certain semantic role, it attempts to comprehend the underlying meaning and connections between them.\n",
    "\n",
    "In order to identify the functions of various constituents in respect to the primary predicate, SRL analyses the syntactic structure of a sentence. Depending on the particular linguistic framework employed, these roles may commonly include agent, patient, instrument, location, time, and a wide variety of others.\n",
    "\n",
    "The following steps are part of the semantic role labelling process:\n",
    "\n",
    "1. Parsing: First, the sentence is parsed to determine its syntactic structure, such as the dependency parse or constituency parse. This step helps identify the relationships between words in the sentence.\n",
    "\n",
    "2. Predicate Identification: The main verb or predicate of the sentence is identified. It serves as the anchor point for assigning roles to other words in the sentence.\n",
    "\n",
    "3. Role Labeling: Each word or phrase in the sentence is assigned a specific role based on its relationship to the main predicate. This is done by considering the syntactic structure, contextual information, and linguistic cues.\n",
    "\n",
    "4. Role Classification: The assigned roles are typically categorized into a predefined set of semantic roles. For example, a noun phrase might be labeled as an agent, indicating that it performs the action denoted by the predicate.\n",
    "\n",
    "5. Disambiguation: In cases where multiple roles are possible for a word or phrase, disambiguation techniques are applied to select the most appropriate role based on the context and semantic constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d37bf95-6730-45f4-8b96-b66f1b4dea07",
   "metadata": {},
   "source": [
    "#### 7. Why Fine-tuning a BERT model takes less time than pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a054afcf-3af6-4bbd-9b50-6c605992629f",
   "metadata": {},
   "source": [
    "Because pretraining entails training a BERT model from scratch on a sizable corpus of unlabeled text, which is a computationally expensive job, fine-tuning a BERT model often takes less time than pretraining. By predicting missing words in a sentence (Masked Language Modelling) and determining whether two phrases follow one another (Next Sentence Prediction), BERT learns general language representations during pretraining.\n",
    "\n",
    "On the other hand, fine-tuning entails using labelled data to tailor the pretrained BERT model to a particular downstream task. Only the extra task-specific layers need to be trained during fine-tuning, while the pre-trained BERT layers must remain frozen or partially updated. The pretrained BERT model has already acquired rich language representations that may be used for a variety of applications, making this procedure quicker.\n",
    "\n",
    "The advantages of fine-tuning over pretraining include:\n",
    "\n",
    "1. Transfer Learning: By starting with a pretrained BERT model, fine-tuning allows for leveraging the knowledge and representations learned during pretraining, which can significantly boost performance on downstream tasks.\n",
    "\n",
    "2. Reduced Training Time: Since the bulk of the model training is already done during pretraining, fine-tuning requires training only the task-specific layers. This reduces the overall training time, making it more efficient.\n",
    "\n",
    "3. Data Efficiency: Fine-tuning requires less labeled data compared to pretraining, as it leverages the knowledge already present in the pretrained model. This can be beneficial in scenarios where labeled data is limited or expensive to obtain.\n",
    "\n",
    "4. Task-Specific Optimization: Fine-tuning allows for adapting the pretrained BERT model to the specific nuances and requirements of the downstream task. The task-specific layers can be optimized to capture the task-specific patterns and improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7fbd3b-a2f6-4af5-87a3-4eec7077ba15",
   "metadata": {},
   "source": [
    "#### 8. Recognizing Textual Entailment (RTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81a5081-57b3-4fc9-9662-20c8b2026ffd",
   "metadata": {},
   "source": [
    "A task in natural language processing called Recognising Textual Entailment (RTE) entails figuring out the connection between two text samples called the \"text\" and the \"hypothesis.\" The objective is to ascertain whether the hypothesis's meaning can be logically or deduced from the text's meaning. \n",
    "\n",
    "RTE divides the relationships between the text and hypotheses into three groups:\n",
    "\n",
    "1. Entailment: The hypothesis can be inferred or logically derived from the text. It means the information in the text supports or implies the information in the hypothesis.\n",
    "\n",
    "2. Contradiction: The hypothesis contradicts or is in direct conflict with the information in the text. The meaning of the hypothesis cannot be true based on the information in the text.\n",
    "\n",
    "3. Neutral: There is no clear logical relationship between the text and the hypothesis. The text does not provide enough information to determine if the hypothesis is true or false."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59647c8-983b-457b-8916-9bbfaa4a8bbf",
   "metadata": {},
   "source": [
    "#### 9. Explain the decoder stack of  GPT models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ceaae2-aa9b-4bf6-b371-ce87c11b9a64",
   "metadata": {},
   "source": [
    "Coherent and contextually appropriate text is produced by the decoder stack of GPT (Generative Pre-trained Transformer) models. In order to acquire and process data from various viewpoints and levels of abstraction, the model's numerous layers of self-attention and feed-forward neural networks are used.\n",
    "\n",
    "The Transformer architecture is often comparable to the decoder stack of GPT models, with each layer consisting of two sub-layers:\n",
    "\n",
    "1. Self-Attention Layer: This layer employs self-attention mechanism to capture dependencies between words within the input sequence. It allows the model to assign different weights to different words based on their relevance and importance in generating the next word. The self-attention mechanism helps the model understand the context and dependencies among words, enabling it to generate more coherent and contextually appropriate text.\n",
    "\n",
    "2. Feed-Forward Neural Network Layer: After the self-attention layer, the output is passed through a feed-forward neural network, which applies non-linear transformations to the representations learned from the previous layer. It helps capture more complex patterns and relationships in the data, further enhancing the model's ability to generate high-quality text.\n",
    "\n",
    "The decoder stack consists of multiple layers of these sub-layers, typically ranging from several to dozens of layers. Each layer allows the model to iteratively refine and improve its understanding of the input sequence, incorporating both local and global context to generate more accurate and meaningful text.\n",
    "\n",
    "The output is typically routed via a linear transformation and a softmax activation function at the top layer of the decoder stack to produce the probability distribution across the vocabulary. The following word in the resulting text is chosen using this distribution.\n",
    "\n",
    "The decoder stack of GPT models uses multiple layers of self-attention and feed-forward neural networks to generate coherent and contextually relevant text, making them useful for tasks like language modelling, text generation, and machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc9567b-710b-4167-89ab-a035eb996cc5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
